{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "## Paper\n",
    "\n",
    "[Kika, Alda, et al. \"Imbalance Node Classification with Graph Neural Networks (GNN): A Study on a Twitter Dataset.\"](https://www.proquest.com/openview/707deabdf2dee201896409a9a4fccfb7/1?pq-origsite=gscholar&cbl=5444811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import BatchLoader, Graph, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from spektral.data import Graph, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"N4J_USER\")\n",
    "client_secret = os.getenv(\"N4J_PW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Reddit Dataset\n",
    "\n",
    "Using Spektral, we have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def read(self):\n",
    "        driver = GraphDatabase.driver(\n",
    "            \"bolt://localhost:7687\", auth=(client_id, client_secret)\n",
    "        )\n",
    "\n",
    "        with driver.session() as session:\n",
    "            disorder_results = session.run(\"MATCH (n:Mental_Health_Disorder) RETURN n.name as name\")\n",
    "            disorders = {\n",
    "                record[\"name\"]: idx for idx, record in enumerate(disorder_results)\n",
    "            }\n",
    "\n",
    "            word_results = session.run(\"MATCH (n:Word) RETURN n.name as name\")\n",
    "            words = {\n",
    "                record[\"name\"]: idx + len(disorders)\n",
    "                for idx, record in enumerate(word_results)\n",
    "            }\n",
    "\n",
    "            verb_results = session.run(\"MATCH (n:Verb) RETURN n.name as name\")\n",
    "            verbs = {record[\"name\"]: idx + len(disorders) + len(words) for idx, record in enumerate(verb_results)}\n",
    "\n",
    "\n",
    "            # edge_results = session.run(\n",
    "            #     \"MATCH (n:Word)-[r]->(m:Subreddit) RETURN n.name as source, m.name as target\"\n",
    "            # )\n",
    "            # edges = [\n",
    "            #     (words[record[\"source\"]], subreddits[record[\"target\"]])\n",
    "            #     for record in edge_results\n",
    "            # ]\n",
    "\n",
    "            \n",
    "            subreddit_verb_results = session.run(\n",
    "                \"MATCH (s:Mental_Health_Disorder)-[r]->(v:Verb) RETURN s.name as source, v.name as target\"\n",
    "            )\n",
    "            disorder_verb_edges = [\n",
    "                (disorders[record[\"source\"]], verbs[record[\"target\"]])\n",
    "                for record in subreddit_verb_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            verb_word_results = session.run(\n",
    "                \"MATCH (v:Verb)-[r]->(w:Word) RETURN v.name as source, w.name as target\"\n",
    "            )\n",
    "            verb_word_edges = [\n",
    "                (verbs[record[\"source\"]], words[record[\"target\"]])\n",
    "                for record in verb_word_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            edges = disorder_verb_edges + verb_word_edges\n",
    "        \n",
    "        \n",
    "        num_nodes = len(disorders) + len(words) + len(verbs)\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for src, dst in edges:\n",
    "            adj_matrix[src][dst] = 1\n",
    "\n",
    "        node_features = np.eye(num_nodes)\n",
    "        labels = np.zeros((num_nodes, 1))\n",
    "\n",
    "        return [Graph(x=node_features, a=adj_matrix, y=labels)]\n",
    "\n",
    "\n",
    "dataset = RedditDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "This is similar to a recommendation problem.\n",
    "\n",
    "See: https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditGNN(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(64, activation=\"relu\")\n",
    "        self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "        self.dense = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 6  #The MH subreddit count.\n",
    "model = RedditGNN(num_classes=num_classes)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Subreddit - Therefore Mental Health Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'gcn_conv_22' (type GCNConv).\n\n{{function_node __wrapped____MklMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [1,13255], In[1]: [1,64] [Op:MatMul] name: \n\nCall arguments received by layer 'gcn_conv_22' (type GCNConv):\n  • inputs=['tf.Tensor(shape=(1, 13255), dtype=float32)', 'tf.Tensor(shape=(1, 13255), dtype=float32)']\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 32\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(\"Shape of x:\", x.shape)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"Shape of a:\", a.shape)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# x = x.T  # Transpose the feature matrix\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# a = a.reshape((num_nodes, num_nodes))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(y, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m---> 32\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([x, a], y_one_hot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[55], line 10\u001b[0m, in \u001b[0;36mRedditGNN.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m      9\u001b[0m     x, a \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1([x, a])\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2([x, a])\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:74\u001b[0m, in \u001b[0;36mcheck_dtypes_decorator.<locals>._inner_check_dtypes\u001b[1;34m(inputs, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(call)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_check_dtypes\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m check_dtypes(inputs)\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py:101\u001b[0m, in \u001b[0;36mGCNConv.call\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m     98\u001b[0m x, a \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m    100\u001b[0m output \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mdot(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel)\n\u001b[1;32m--> 101\u001b[0m output \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmodal_dot(a, output)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    104\u001b[0m     output \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mbias_add(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\spektral\\layers\\ops\\matmul.py:131\u001b[0m, in \u001b[0;36mmodal_dot\u001b[1;34m(a, b, transpose_a, transpose_b)\u001b[0m\n\u001b[0;32m    127\u001b[0m     b \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtranspose(b, perm)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_ndim \u001b[38;5;241m==\u001b[39m b_ndim:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# ...ij,...jk->...ik\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot(a, b)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m a_ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# ij,bjk->bik\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mixed_mode_dot(a, b)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\spektral\\layers\\ops\\matmul.py:58\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Handle case: rank 2 dense-dense, rank 3 dense-dense\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Here we use the standard dense operation\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmatmul(a, b)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'gcn_conv_22' (type GCNConv).\n\n{{function_node __wrapped____MklMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [1,13255], In[1]: [1,64] [Op:MatMul] name: \n\nCall arguments received by layer 'gcn_conv_22' (type GCNConv):\n  • inputs=['tf.Tensor(shape=(1, 13255), dtype=float32)', 'tf.Tensor(shape=(1, 13255), dtype=float32)']\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def predict_subreddit(model, words, all_subreddits):\n",
    "    word_embeddings = model.predict(words)  # Shape: [num_words, embedding_dim]\n",
    "    subreddit_embeddings = model.predict(all_subreddits)  # Shape: [num_subreddits, embedding_dim]\n",
    "    similarity_matrix = cosine_similarity(word_embeddings, subreddit_embeddings)  # Shape: [num_words, num_subreddits]\n",
    "\n",
    "    closest_subreddits_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    closest_subreddits = [all_subreddits[idx] for idx in closest_subreddits_indices]\n",
    "\n",
    "    return closest_subreddits\n",
    "\n",
    "\n",
    "# Christian: We need to fit model first! \n",
    "\n",
    "# Adam: I need embeddings.\n",
    "word_embeddings = None  # TODO\n",
    "subreddit_embeddings = None  # TODO\n",
    "closest_subreddits = predict_subreddit(model, word_embeddings, subreddit_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
