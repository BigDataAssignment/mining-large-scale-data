{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "Author: Adam Darmanin\n",
    "\n",
    "## Paper\n",
    "\n",
    "[Kika, Alda, et al. \"Imbalance Node Classification with Graph Neural Networks (GNN): A Study on a Twitter Dataset.\"](https://www.proquest.com/openview/707deabdf2dee201896409a9a4fccfb7/1?pq-origsite=gscholar&cbl=5444811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import BatchLoader, Graph, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from spektral.data import Graph, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"N4J_USER\")\n",
    "client_secret = os.getenv(\"N4J_PW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Reddit Dataset\n",
    "\n",
    "Using Spektral, we have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def read(self):\n",
    "        driver = GraphDatabase.driver(\n",
    "            \"bolt://localhost:7687\", auth=(client_id, client_secret)\n",
    "        )\n",
    "\n",
    "        with driver.session() as session:\n",
    "            node_results = session.run(\"MATCH (n:Subreddit) RETURN n.name as name\")\n",
    "            subreddits = {\n",
    "                record[\"name\"]: idx for idx, record in enumerate(node_results)\n",
    "            }\n",
    "\n",
    "            word_results = session.run(\"MATCH (n:Word) RETURN n.name as name\")\n",
    "            words = {\n",
    "                record[\"name\"]: idx + len(subreddits)\n",
    "                for idx, record in enumerate(word_results)\n",
    "            }\n",
    "\n",
    "            edge_results = session.run(\n",
    "                \"MATCH (n:Word)-[r]->(m:Subreddit) RETURN n.name as source, m.name as target\"\n",
    "            )\n",
    "            edges = [\n",
    "                (words[record[\"source\"]], subreddits[record[\"target\"]])\n",
    "                for record in edge_results\n",
    "            ]\n",
    "\n",
    "        num_nodes = len(subreddits)\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for src, dst in edges:\n",
    "            adj_matrix[src][dst] = 1\n",
    "\n",
    "        node_features = np.eye(num_nodes)\n",
    "        labels = np.zeros((num_nodes, 1))\n",
    "\n",
    "        return [Graph(x=node_features, a=adj_matrix, y=labels)]\n",
    "\n",
    "\n",
    "dataset = RedditDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "This is similar to a recommendation problem.\n",
    "\n",
    "See: https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditGNN(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(64, activation=\"relu\")\n",
    "        self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "        self.dense = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 1  # Adam: The MH subreddit count.\n",
    "\n",
    "model = RedditGNN(num_classes=num_classes)\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Subreddit - Therefore Mental Health Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m     16\u001b[0m subreddit_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m closest_subreddits \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_subreddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubreddit_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mpredict_subreddit\u001b[1;34m(model, words, all_subreddits)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_subreddit\u001b[39m(model, words, all_subreddits):\n\u001b[1;32m----> 4\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [num_words, embedding_dim]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     subreddit_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(all_subreddits)  \u001b[38;5;66;03m# Shape: [num_subreddits, embedding_dim]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     similarity_matrix \u001b[38;5;241m=\u001b[39m cosine_similarity(word_embeddings, subreddit_embeddings)  \u001b[38;5;66;03m# Shape: [num_words, num_subreddits]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\adamd\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_subreddit(model, words, all_subreddits):\n",
    "    word_embeddings = model.predict(words)  # Shape: [num_words, embedding_dim]\n",
    "    subreddit_embeddings = model.predict(all_subreddits)  # Shape: [num_subreddits, embedding_dim]\n",
    "    similarity_matrix = cosine_similarity(word_embeddings, subreddit_embeddings)  # Shape: [num_words, num_subreddits]\n",
    "\n",
    "    closest_subreddits_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    closest_subreddits = [all_subreddits[idx] for idx in closest_subreddits_indices]\n",
    "\n",
    "    return closest_subreddits\n",
    "\n",
    "\n",
    "# Adam: I need embeddings.\n",
    "word_embeddings = None  # TODO\n",
    "subreddit_embeddings = None  # TODO\n",
    "closest_subreddits = predict_subreddit(model, word_embeddings, subreddit_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
