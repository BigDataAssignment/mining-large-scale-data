{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "Author: Adam Darmanin\n",
    "\n",
    "## Paper\n",
    "\n",
    "[Kika, Alda, et al. \"Imbalance Node Classification with Graph Neural Networks (GNN): A Study on a Twitter Dataset.\"](https://www.proquest.com/openview/707deabdf2dee201896409a9a4fccfb7/1?pq-origsite=gscholar&cbl=5444811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import BatchLoader, Graph, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from spektral.data import Graph, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"N4J_USER\")\n",
    "client_secret = os.getenv(\"N4J_PW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Reddit Dataset\n",
    "\n",
    "Using Spektral, we have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def read(self):\n",
    "        driver = GraphDatabase.driver(\n",
    "            \"bolt://localhost:7687\", auth=(client_id, client_secret)\n",
    "        )\n",
    "\n",
    "        with driver.session() as session:\n",
    "            disorder_results = session.run(\"MATCH (n:Mental_Health_Disorder) RETURN n.name as name\")\n",
    "            disorders = {\n",
    "                record[\"name\"]: idx for idx, record in enumerate(disorder_results)\n",
    "            }\n",
    "\n",
    "            word_results = session.run(\"MATCH (n:Word) RETURN n.name as name\")\n",
    "            words = {\n",
    "                record[\"name\"]: idx + len(disorders)\n",
    "                for idx, record in enumerate(word_results)\n",
    "            }\n",
    "\n",
    "            verb_results = session.run(\"MATCH (n:Verb) RETURN n.name as name\")\n",
    "            verbs = {record[\"name\"]: idx + len(disorders) + len(words) for idx, record in enumerate(verb_results)}\n",
    "\n",
    "\n",
    "            # edge_results = session.run(\n",
    "            #     \"MATCH (n:Word)-[r]->(m:Subreddit) RETURN n.name as source, m.name as target\"\n",
    "            # )\n",
    "            # edges = [\n",
    "            #     (words[record[\"source\"]], subreddits[record[\"target\"]])\n",
    "            #     for record in edge_results\n",
    "            # ]\n",
    "\n",
    "            \n",
    "            subreddit_verb_results = session.run(\n",
    "                \"MATCH (s:Mental_Health_Disorder)-[r]->(v:Verb) RETURN s.name as source, v.name as target\"\n",
    "            )\n",
    "            disorder_verb_edges = [\n",
    "                (disorders[record[\"source\"]], verbs[record[\"target\"]])\n",
    "                for record in subreddit_verb_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            verb_word_results = session.run(\n",
    "                \"MATCH (v:Verb)-[r]->(w:Word) RETURN v.name as source, w.name as target\"\n",
    "            )\n",
    "            verb_word_edges = [\n",
    "                (verbs[record[\"source\"]], words[record[\"target\"]])\n",
    "                for record in verb_word_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            edges = disorder_verb_edges + verb_word_edges\n",
    "        \n",
    "        \n",
    "        \n",
    "        num_nodes = len(disorders) + len(words) + len(verbs)\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for src, dst in edges:\n",
    "            adj_matrix[src][dst] = 1\n",
    "\n",
    "        node_features = np.eye(num_nodes)\n",
    "        labels = np.zeros((num_nodes, 1))\n",
    "\n",
    "        return [Graph(x=node_features, a=adj_matrix, y=labels)]\n",
    "\n",
    "\n",
    "dataset = RedditDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "This is similar to a recommendation problem.\n",
    "\n",
    "See: https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditGNN(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(64, activation=\"relu\")\n",
    "        self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "        self.dense = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 1  # Adam: The MH subreddit count.\n",
    "\n",
    "model = RedditGNN(num_classes=num_classes)\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Subreddit - Therefore Mental Health Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '_assert_compile_was_called'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m graph_data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     21\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m: a}\n\u001b[1;32m---> 22\u001b[0m RedditGNN\u001b[38;5;241m.\u001b[39mfit(input_data, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1710\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;66;03m# Legacy graph support is contained in `training_v1.Model`.\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m version_utils\u001b[38;5;241m.\u001b[39mdisallow_legacy_graph(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1710\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_compile_was_called()\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1712\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute '_assert_compile_was_called'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_subreddit(model, words, all_subreddits):\n",
    "    word_embeddings = model.predict(words)  # Shape: [num_words, embedding_dim]\n",
    "    subreddit_embeddings = model.predict(all_subreddits)  # Shape: [num_subreddits, embedding_dim]\n",
    "    similarity_matrix = cosine_similarity(word_embeddings, subreddit_embeddings)  # Shape: [num_words, num_subreddits]\n",
    "\n",
    "    closest_subreddits_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    closest_subreddits = [all_subreddits[idx] for idx in closest_subreddits_indices]\n",
    "\n",
    "    return closest_subreddits\n",
    "\n",
    "\n",
    "\n",
    "# dataset = RedditDataset()\n",
    "# graph_data = dataset[0] \n",
    "# x = graph_data.x.astype(np.float32)  \n",
    "# a = graph_data.a.astype(np.float32) \n",
    "# y = graph_data.y.astype(np.float32)\n",
    "\n",
    "# input_data = {'x': x, 'a': a}\n",
    "# RedditGNN.fit(input_data, y, epochs=10, batch_size=1)\n",
    "\n",
    "\n",
    "# Adam: I need embeddings.\n",
    "word_embeddings = None  # TODO\n",
    "subreddit_embeddings = None  # TODO\n",
    "closest_subreddits = predict_subreddit(model, word_embeddings, subreddit_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
