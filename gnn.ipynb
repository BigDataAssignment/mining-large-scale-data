{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "Author: Adam Darmanin\n",
    "\n",
    "## Paper\n",
    "\n",
    "[Kika, Alda, et al. \"Imbalance Node Classification with Graph Neural Networks (GNN): A Study on a Twitter Dataset.\"](https://www.proquest.com/openview/707deabdf2dee201896409a9a4fccfb7/1?pq-origsite=gscholar&cbl=5444811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import BatchLoader, Graph, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from spektral.data import Graph, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"N4J_USER\")\n",
    "client_secret = os.getenv(\"N4J_PW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Reddit Dataset\n",
    "\n",
    "Using Spektral, we have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def read(self):\n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(\n",
    "            \"bolt://localhost:7687\", auth=(client_id, client_secret)\n",
    "        )\n",
    "\n",
    "        with driver.session() as session:\n",
    "            # Get Subreddits\n",
    "            node_results = session.run(\"MATCH (n:Subreddit) RETURN n.name as name\")\n",
    "            subreddits = {\n",
    "                record[\"name\"]: idx for idx, record in enumerate(node_results)\n",
    "            }\n",
    "\n",
    "            # Get Words\n",
    "            word_results = session.run(\"MATCH (n:Word) RETURN n.name as name\")\n",
    "            words = {\n",
    "                record[\"name\"]: idx + len(subreddits)\n",
    "                for idx, record in enumerate(word_results)\n",
    "            }\n",
    "\n",
    "            # Get Edges\n",
    "            edge_results = session.run(\n",
    "                \"MATCH (n:Word)-[r]->(m:Subreddit) RETURN n.name as source, m.name as target\"\n",
    "            )\n",
    "            edges = [\n",
    "                (words[record[\"source\"]], subreddits[record[\"target\"]])\n",
    "                for record in edge_results\n",
    "            ]\n",
    "\n",
    "        # Create adjacency matrix\n",
    "        num_nodes = len(subreddits) + len(words)\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for src, dst in edges:\n",
    "            adj_matrix[src][dst] = 1  # Assuming undirected graph\n",
    "\n",
    "        # Placeholder for node features and labels\n",
    "        node_features = np.eye(num_nodes)  # One-hot encoding for simplicity\n",
    "        labels = np.zeros(\n",
    "            (num_nodes, 1)\n",
    "        )  # You will need to define labels appropriately\n",
    "\n",
    "        return [Graph(x=node_features, a=adj_matrix, y=labels)]\n",
    "\n",
    "\n",
    "dataset = RedditDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "This is similar to a recommendation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditGNN(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(64, activation=\"relu\")\n",
    "        self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "        self.dense = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "# Assuming num_classes is the number of unique labels for your nodes\n",
    "num_classes = 2  # Replace with your actual number of classes\n",
    "model = RedditGNN(num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Subreddit - Therefore Mental Health Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_subreddit(model, word_embeddings, subreddit_embeddings):\n",
    "    # Calculate cosine similarity or another metric\n",
    "    similarity_matrix = np.dot(word_embeddings, subreddit_embeddings.T)\n",
    "    closest_subreddits = np.argmax(similarity_matrix, axis=1)\n",
    "    return closest_subreddits\n",
    "\n",
    "\n",
    "# Example usage (assuming you have word_embeddings and subreddit_embeddings prepared)\n",
    "# word_embeddings = ...  # Obtain embeddings for input words\n",
    "# subreddit_embeddings = ...  # Obtain embeddings for all subreddits\n",
    "# closest_subreddits = predict_subreddit(model, word_embeddings, subreddit_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
