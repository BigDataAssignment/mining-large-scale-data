{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "Author: Adam Darmanin\n",
    "\n",
    "## Paper\n",
    "\n",
    "[Kika, Alda, et al. \"Imbalance Node Classification with Graph Neural Networks (GNN): A Study on a Twitter Dataset.\"](https://www.proquest.com/openview/707deabdf2dee201896409a9a4fccfb7/1?pq-origsite=gscholar&cbl=5444811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import BatchLoader, Graph, Dataset\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from spektral.data import Graph, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"N4J_USER\")\n",
    "client_secret = os.getenv(\"N4J_PW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our Reddit Dataset\n",
    "\n",
    "Using Spektral, we have our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditDataset(Dataset):\n",
    "    def read(self):\n",
    "        driver = GraphDatabase.driver(\n",
    "            \"bolt://localhost:7687\", auth=(client_id, client_secret)\n",
    "        )\n",
    "\n",
    "        with driver.session() as session:\n",
    "            disorder_results = session.run(\"MATCH (n:Mental_Health_Disorder) RETURN n.name as name\")\n",
    "            disorders = {\n",
    "                record[\"name\"]: idx for idx, record in enumerate(disorder_results)\n",
    "            }\n",
    "\n",
    "            word_results = session.run(\"MATCH (n:Word) RETURN n.name as name\")\n",
    "            words = {\n",
    "                record[\"name\"]: idx + len(disorders)\n",
    "                for idx, record in enumerate(word_results)\n",
    "            }\n",
    "\n",
    "            verb_results = session.run(\"MATCH (n:Verb) RETURN n.name as name\")\n",
    "            verbs = {record[\"name\"]: idx + len(disorders) + len(words) for idx, record in enumerate(verb_results)}\n",
    "\n",
    "\n",
    "            # edge_results = session.run(\n",
    "            #     \"MATCH (n:Word)-[r]->(m:Subreddit) RETURN n.name as source, m.name as target\"\n",
    "            # )\n",
    "            # edges = [\n",
    "            #     (words[record[\"source\"]], subreddits[record[\"target\"]])\n",
    "            #     for record in edge_results\n",
    "            # ]\n",
    "\n",
    "            \n",
    "            subreddit_verb_results = session.run(\n",
    "                \"MATCH (s:Mental_Health_Disorder)-[r]->(v:Verb) RETURN s.name as source, v.name as target\"\n",
    "            )\n",
    "            disorder_verb_edges = [\n",
    "                (disorders[record[\"source\"]], verbs[record[\"target\"]])\n",
    "                for record in subreddit_verb_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            verb_word_results = session.run(\n",
    "                \"MATCH (v:Verb)-[r]->(w:Word) RETURN v.name as source, w.name as target\"\n",
    "            )\n",
    "            verb_word_edges = [\n",
    "                (verbs[record[\"source\"]], words[record[\"target\"]])\n",
    "                for record in verb_word_results\n",
    "            ]\n",
    "\n",
    "            \n",
    "            edges = disorder_verb_edges + verb_word_edges\n",
    "        \n",
    "        \n",
    "        \n",
    "        num_nodes = len(disorders) + len(words) + len(verbs)\n",
    "        adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "        for src, dst in edges:\n",
    "            adj_matrix[src][dst] = 1\n",
    "\n",
    "        node_features = np.eye(num_nodes)\n",
    "        labels = np.zeros((num_nodes, 1))\n",
    "\n",
    "        return [Graph(x=node_features, a=adj_matrix, y=labels)]\n",
    "\n",
    "\n",
    "dataset = RedditDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "\n",
    "This is similar to a recommendation problem.\n",
    "\n",
    "See: https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditGNN(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = GCNConv(64, activation=\"relu\")\n",
    "        self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "        self.dense = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        return self.dense(x)\n",
    "\n",
    "num_classes = 6  #The MH subreddit count.\n",
    "model = RedditGNN(num_classes=num_classes)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Subreddit - Therefore Mental Health Issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
