{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install readability-lxml\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load EN from None\n",
      "doing ./data/depression-sample - ./data/suicidewatch-sample\n",
      "choosing min from 857 555\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6675829286883809\n",
      "Precision: 0.6491467417212969\n",
      "Recall: 0.6907656160006971\n",
      "Confusion matrix:\n",
      "[[383 172]\n",
      " [209 346]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6497622278254247\n",
      "Precision: 0.6585062976000725\n",
      "Recall: 0.6457898329074913\n",
      "Confusion matrix:\n",
      "[[358 197]\n",
      " [187 368]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6438896082445703\n",
      "Precision: 0.6436733817003066\n",
      "Recall: 0.6505365626668628\n",
      "Confusion matrix:\n",
      "[[360 195]\n",
      " [202 353]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6822087095676038\n",
      "Precision: 0.6857396433865875\n",
      "Recall: 0.685452352699941\n",
      "Confusion matrix:\n",
      "[[380 175]\n",
      " [179 376]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6403739509758647\n",
      "Precision: 0.6936832673387259\n",
      "Recall: 0.6029099395998726\n",
      "Confusion matrix:\n",
      "[[333 222]\n",
      " [151 404]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6503620523691335\n",
      "Precision: 0.6660481494650646\n",
      "Recall: 0.6387164474113046\n",
      "Confusion matrix:\n",
      "[[354 201]\n",
      " [180 375]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6415300154649489\n",
      "Precision: 0.6522864391440187\n",
      "Recall: 0.6531329145661448\n",
      "Confusion matrix:\n",
      "[[358 197]\n",
      " [199 356]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6464688427068251\n",
      "Precision: 0.6823077324649652\n",
      "Recall: 0.6315300031157288\n",
      "Confusion matrix:\n",
      "[[348 207]\n",
      " [171 384]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6343644180938346\n",
      "Precision: 0.6650861335497665\n",
      "Recall: 0.6251089797467188\n",
      "Confusion matrix:\n",
      "[[342 213]\n",
      " [182 373]]\n",
      "got 1110 records...training\n",
      "Total documents classified: 1110\n",
      "Score: 0.6870077375994463\n",
      "Precision: 0.6697465475032084\n",
      "Recall: 0.7078913112591262\n",
      "Confusion matrix:\n",
      "[[393 162]\n",
      " [193 362]]\n",
      "RESULTS: ('./data/depression-sample', './data/suicidewatch-sample') [0.6675829286883809, 0.6497622278254247, 0.6438896082445703, 0.6822087095676038, 0.6403739509758647, 0.6503620523691335, 0.6415300154649489, 0.6464688427068251, 0.6343644180938346, 0.6870077375994463]\n",
      "finished ('./data/depression-sample', './data/suicidewatch-sample')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from readability import Document\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PoS = [\"NN\", \"PRP\", \"IN\", \"DT\", \"RB\", \"JJ\", \"VB\", \"CC\", \"NNS\", \"VBP\"]\n",
    "FirstPersonPronouns = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"mine\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"us\",\n",
    "    \"we\",\n",
    "]\n",
    "\n",
    "if \"SPACY_DATA\" in os.environ:\n",
    "    data_dir = os.environ[\"SPACY_DATA\"]\n",
    "else:\n",
    "    data_dir = None\n",
    "print(\"Load EN from %s\" % data_dir)\n",
    "\n",
    "nlp = English(data_dir=data_dir)\n",
    "\n",
    "\n",
    "def getTextFromRecord(row):\n",
    "    if not pd.isnull(row[\"body\"]):\n",
    "        rs = row[\"body\"]\n",
    "    else:\n",
    "        rs = row[\"selftext\"]\n",
    "    if rs == \"[deleted]\":\n",
    "        return None\n",
    "    return rs\n",
    "\n",
    "\n",
    "def addLexicalFeatures(df):\n",
    "    from collections import Counter\n",
    "\n",
    "    df[\"text\"] = df.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    def cosineSimilarity(sentence1, sentence2, NounsOrPronounsOnly=False):\n",
    "        import math\n",
    "\n",
    "        if not NounsOrPronounsOnly:\n",
    "            vector1 = Counter([token.text.lower() for token in sentence1])\n",
    "            vector2 = Counter([token.text.lower() for token in sentence2])\n",
    "        else:\n",
    "            vector1 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence1\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "            vector2 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence2\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "        intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "        numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
    "        sum1 = sum([vector1[x] ** 2 for x in vector1.keys()])\n",
    "        sum2 = sum([vector2[x] ** 2 for x in vector2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "\n",
    "    def getDocumentSimilarity(text, NounsOrPronounsOnly=False):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "        rs = 0\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            rs += cosineSimilarity(sentences[i], sentences[i + 1], NounsOrPronounsOnly)\n",
    "        rs = rs / float(len(sentences) - 1)\n",
    "        return rs\n",
    "\n",
    "    def getPronounsCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        pronouns = []\n",
    "        for sentence in sentences:\n",
    "            pronouns.extend(\n",
    "                [token.text for token in sentence if token.tag_.startswith(\"PRP\")]\n",
    "            )\n",
    "\n",
    "        pronouns = Counter(pronouns)\n",
    "        pronounsNo = np.sum(list(pronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"sentencesNo\": len(sentences),\n",
    "                \"pronouns\": pronouns,\n",
    "                \"pronounsNo\": pronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def getDefiniteArticlesCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        definiteArticles = []\n",
    "        for sentence in sentences:\n",
    "            definiteArticles.extend(\n",
    "                [\n",
    "                    token.text\n",
    "                    for token in sentence\n",
    "                    if (token.tag_ == \"DT\" and token.text.lower() == \"the\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        definiteArticles = Counter(definiteArticles)\n",
    "        definiteArticlesNo = np.sum(list(definiteArticles.values()))\n",
    "        return pd.Series({\"definiteArticlesNo\": definiteArticlesNo})\n",
    "\n",
    "    def getFirstPersonPronounsCounter(counter):\n",
    "        l = [k for k in counter.keys() if k.lower() in FirstPersonPronouns]\n",
    "        rs = {}\n",
    "        for k in l:\n",
    "            rs[k] = counter[k]\n",
    "        firstPersonPronouns = Counter(rs)\n",
    "        firstPersonPronounsNo = np.sum(list(firstPersonPronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"firstPersonPronouns\": firstPersonPronouns,\n",
    "                \"firstPersonPronounsNo\": firstPersonPronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df[\"documentSimilarity\"] = df[\"text\"].apply(getDocumentSimilarity)\n",
    "    df[\"documentSimilarityNounsOrPronouns\"] = df[\"text\"].apply(\n",
    "        getDocumentSimilarity, args=(True,)\n",
    "    )\n",
    "    df[[\"pronouns\", \"pronounsNo\", \"sentencesNo\"]] = df[\"text\"].apply(getPronounsCounter)\n",
    "    df[[\"definiteArticlesNo\"]] = df[\"text\"].apply(getDefiniteArticlesCounter)\n",
    "    df[[\"firstPersonPronouns\", \"firstPersonPronounsNo\"]] = df[\"pronouns\"].apply(\n",
    "        getFirstPersonPronounsCounter\n",
    "    )\n",
    "    df[\"firstPersonPronounsRatio\"] = df[\"firstPersonPronounsNo\"] / df[\n",
    "        \"pronounsNo\"\n",
    "    ].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getSyntacticFeatures(row):\n",
    "    text = getTextFromRecord(row)\n",
    "\n",
    "    def getHeightToken(token):\n",
    "        height = 1\n",
    "        while token != token.head:\n",
    "            height += 1\n",
    "            token = token.head\n",
    "        return height\n",
    "\n",
    "    def getVerbPhrasesLength(sentence):\n",
    "        rs = [0]\n",
    "        inVerb = False\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if not inVerb:\n",
    "                    i = 1\n",
    "                inVerb = True\n",
    "                i += 1\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if inVerb:\n",
    "                    rs.append(i - 1)\n",
    "                inVerb = False\n",
    "        return rs\n",
    "\n",
    "    if text is None:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"maxHeight\": np.nan,\n",
    "                \"noun_chunks\": np.nan,\n",
    "                \"maxVerbPhraseLength\": np.nan,\n",
    "                \"subordinateConjuctions\": np.nan,\n",
    "            },\n",
    "            dtype=object,\n",
    "        )\n",
    "    doc = nlp(str(text))\n",
    "    noun_chunks = len(list(doc.noun_chunks))\n",
    "    sentences = list(doc.sents)\n",
    "    maxHeight = 1\n",
    "\n",
    "    subordinateConjuctions = 0\n",
    "    for sentence in sentences:\n",
    "        subordinateConjuctions += len(\n",
    "            [token for token in sentence if token.tag_ == \"IN\"]\n",
    "        )\n",
    "    subordinateConjuctions = subordinateConjuctions / float(len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        height = 1\n",
    "        if sentence.end == 0:\n",
    "            continue\n",
    "        sentenceHeight = max([getHeightToken(token) for token in sentence])\n",
    "        if maxHeight < sentenceHeight:\n",
    "            maxHeight = sentenceHeight\n",
    "\n",
    "    maxVerbPhraseLength = max(\n",
    "        max([getVerbPhrasesLength(sentence) for sentence in sentences])\n",
    "    )\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"maxHeight\": maxHeight - 1,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"maxVerbPhraseLength\": maxVerbPhraseLength,\n",
    "            \"subordinateConjuctions\": subordinateConjuctions,\n",
    "        },\n",
    "        dtype=object,\n",
    "    )\n",
    "\n",
    "\n",
    "def getURLtoPostRatio(df):\n",
    "    def containedInURL(row):\n",
    "        return row[\"permalink\"] not in row[\"url\"]\n",
    "\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    cnt = len(posts)\n",
    "    urls = len(posts[posts.apply(containedInURL, axis=1)])\n",
    "    return urls / float(cnt)\n",
    "\n",
    "\n",
    "# def getData(df1, df2):\n",
    "#     import readability\n",
    "#     df1 = readability.prepare(df1)\n",
    "#     df2 = readability.prepare(df2)\n",
    "\n",
    "#     if \"text\" not in df1.columns:\n",
    "#         df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "#     if \"text\" not in df2.columns:\n",
    "#         df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "#     df1[\"class\"] = \"class1\"\n",
    "#     df2[\"class\"] = \"class2\"\n",
    "\n",
    "#     df1 = df1[[\"text\", \"class\"]]\n",
    "#     df2 = df2[[\"text\", \"class\"]]\n",
    "#     data = pd.concat([df1, df2])\n",
    "#     data = data.reset_index()\n",
    "#     del data[\"index\"]\n",
    "#     data = data.reindex(np.random.permutation(data.index))\n",
    "#     return data\n",
    "\n",
    "\n",
    "def preprocess_record(record):\n",
    "    doc = Document(record)\n",
    "    return doc.summary()\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df['text'] = df['text'].apply(preprocess_record)\n",
    "    return df\n",
    "\n",
    "def getData(df1, df2):\n",
    "    # Assuming 'html_column' contains HTML content in your DataFrames\n",
    "    df1_processed = preprocess_dataframe(df1)\n",
    "    df2_processed = preprocess_dataframe(df2)\n",
    "\n",
    "    if \"text\" not in df1_processed.columns:\n",
    "        df1_processed[\"text\"] = df1_processed.apply(getTextFromRecord, axis=1)\n",
    "    if \"text\" not in df2_processed.columns:\n",
    "        df2_processed[\"text\"] = df2_processed.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    df1_processed[\"class\"] = \"class1\"\n",
    "    df2_processed[\"class\"] = \"class2\"\n",
    "\n",
    "    df1_processed = df1_processed[[\"text\", \"class\"]]\n",
    "    df2_processed = df2_processed[[\"text\", \"class\"]]\n",
    "\n",
    "    data = pd.concat([df1_processed, df2_processed])\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def bagOfWords(ngram_range=(2, 2)):\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"count_vectorizer\",\n",
    "                CountVectorizer(ngram_range=ngram_range, analyzer=\"word\"),\n",
    "            ),\n",
    "            (\"classifier\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(pipeline, data, getScore=False):\n",
    "    k_fold = KFold(n_splits=6)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for train_indices, test_indices in k_fold.split(data):\n",
    "        train_text = data.iloc[train_indices][\"text\"].values\n",
    "        train_y = data.iloc[train_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices][\"text\"].values\n",
    "        test_y = data.iloc[test_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=\"class1\")\n",
    "        precision += precision_score(test_y, predictions, pos_label=\"class1\")\n",
    "        recall += recall_score(test_y, predictions, pos_label=\"class1\")\n",
    "        scores.append(score)\n",
    "\n",
    "    print(\"Total documents classified:\", len(data))\n",
    "    print(\"Score:\", sum(scores) / len(scores))\n",
    "    print(\"Precision:\", precision / len(scores))\n",
    "    print(\"Recall:\", recall / len(scores))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion)\n",
    "    if getScore:\n",
    "        return pipeline, sum(scores) / float(len(scores))\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def readResults(deviations=False):\n",
    "    df = pd.read_pickle(\"./data/combinations-10fold.pkl\")\n",
    "    df = pd.DataFrame(df)\n",
    "    for column in df.columns:\n",
    "        for index in df.index:\n",
    "            tmp = df[column][index]\n",
    "            if tmp is None:\n",
    "                continue\n",
    "            if deviations:\n",
    "                df[column][index] = np.std(tmp)\n",
    "            else:\n",
    "                df[column][index] = np.mean(tmp)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#binaryClassification.py\n",
    "\n",
    "subreddits = [\"./data/suicidewatch-sample\", \"./data/depression-sample\"]\n",
    "subreddits.sort()\n",
    "\n",
    "\n",
    "ngram_range = (1, 2)\n",
    "fname = \"./data/combinations-10fold.pkl\"\n",
    "combinations = list(itertools.combinations(subreddits, 2))\n",
    "\n",
    "for combination in combinations:\n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname, \"rb\") as f:\n",
    "            rs = pickle.load(f)\n",
    "    else:\n",
    "        rs = {}\n",
    "\n",
    "    # if combination[0] in rs.keys():\n",
    "    #     if combination[1] in rs[combination[0]].keys():\n",
    "    #         print(combination, \"already exists, skipping...\")\n",
    "    #         continue\n",
    "\n",
    "    print(\"doing\", combination[0], \"-\", combination[1])\n",
    "    df1 = pd.read_pickle(combination[0] + \".pkl\")\n",
    "    df1 = df1.reset_index()\n",
    "    df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    df1 = df1.reindex(np.random.permutation(df1.index))\n",
    "    df2 = pd.read_pickle(combination[1] + \".pkl\")\n",
    "    df2 = df2.reset_index()\n",
    "    df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df2 = df2.reindex(np.random.permutation(df2.index))\n",
    "    # keep only posts, keep only the text column\n",
    "    df1 = df1[df1[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df1 = df1.dropna(subset=[\"text\"])\n",
    "    df2 = df2[df2[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df2 = df2.dropna(subset=[\"text\"])\n",
    "\n",
    "    results = []\n",
    "    print(\"choosing min from\", len(df1), len(df2))\n",
    "    m = min(len(df1), len(df2))\n",
    "    for i in range(0, 10):\n",
    "        df1_min = df1.reindex(np.random.permutation(df1.index)).head(m)\n",
    "        df2_min = df2.reindex(np.random.permutation(df2.index)).head(m)\n",
    "\n",
    "        data = getData(df1_min, df2_min)\n",
    "        print(\"got\", len(data), \"records...training\")\n",
    "        pipeline = bagOfWords(ngram_range=ngram_range)\n",
    "        pipeline, score = evaluate(pipeline, data=data, getScore=True)\n",
    "        results.append(score)\n",
    "\n",
    "    print(\"RESULTS:\", combination, results)\n",
    "    if combination[0] not in rs.keys():\n",
    "        rs[combination[0]] = {}\n",
    "    rs[combination[0]][combination[1]] = results\n",
    "\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(rs, f)\n",
    "    print(\"finished\", combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
