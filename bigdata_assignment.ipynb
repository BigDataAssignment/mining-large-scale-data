{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import pandas\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# import content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PoS = [\"NN\", \"PRP\", \"IN\", \"DT\", \"RB\", \"JJ\", \"VB\", \"CC\", \"NNS\", \"VBP\"]\n",
    "FirstPersonPronouns = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"mine\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"us\",\n",
    "    \"we\",\n",
    "]\n",
    "\n",
    "if os.environ.get(\"SPACY_DATA\"):\n",
    "    data_dir = os.environ.get(\"SPACY_DATA\")\n",
    "else:\n",
    "    data_dir = None\n",
    "print(\"Load EN from %s\" % data_dir)\n",
    "\n",
    "# from spacy.en import English\n",
    "# nlp = English(data_dir=data_dir)\n",
    "\n",
    "\n",
    "def getTextFromRecord(row):\n",
    "    if not pandas.isnull(row[\"body\"]):\n",
    "        rs = row[\"body\"]\n",
    "    else:\n",
    "        rs = row[\"selftext\"]\n",
    "    if rs == \"[deleted]\":\n",
    "        return None\n",
    "    return rs\n",
    "\n",
    "\n",
    "def addLexicalFeatures(df):\n",
    "    from collections import Counter\n",
    "\n",
    "    df[\"text\"] = df.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    def cosineSimilarity(sentence1, sentence2, NounsOrPronounesOnly=False):\n",
    "        import math\n",
    "\n",
    "        if NounsOrPronounesOnly is False:\n",
    "            vector1 = Counter([token.text.lower() for token in sentence1])\n",
    "            vector2 = Counter([token.text.lower() for token in sentence2])\n",
    "        else:\n",
    "            vector1 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence1\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "            vector2 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence2\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "        intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "        numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
    "        sum1 = sum([vector1[x] ** 2 for x in vector1.keys()])\n",
    "        sum2 = sum([vector2[x] ** 2 for x in vector2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "\n",
    "    def getDocumentSimilarity(text, NounsOrPronounesOnly=False):\n",
    "        doc = nlp(unicode(text))\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "        i = 0\n",
    "        rs = 0\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            rs += cosineSimilarity(sentences[i], sentences[i + 1], NounsOrPronounesOnly)\n",
    "        rs = rs / float(len(sentences) - 1)\n",
    "        return rs\n",
    "\n",
    "    def getPronounsCounter(text):\n",
    "        doc = nlp(unicode(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        pronouns = []\n",
    "        for sentence in sentences:\n",
    "            pronouns.extend(\n",
    "                [token.text for token in sentence if token.tag_.startswith(\"PRP\")]\n",
    "            )\n",
    "\n",
    "        pronouns = Counter(pronouns)\n",
    "        pronounsNo = np.sum(pronouns.values())\n",
    "        return pandas.Series(\n",
    "            {\n",
    "                \"sentencesNo\": len(sentences),\n",
    "                \"pronouns\": pronouns,\n",
    "                \"pronounsNo\": pronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def getDefiniteArticlesCounter(text):\n",
    "        doc = nlp(unicode(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        definiteArticles = []\n",
    "        for sentence in sentences:\n",
    "            definiteArticles.extend(\n",
    "                [\n",
    "                    token.text\n",
    "                    for token in sentence\n",
    "                    if (token.tag_ == \"DT\" and token.text.lower() == \"the\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        definiteArticles = Counter(definiteArticles)\n",
    "        definiteArticlesNo = np.sum(definiteArticles.values())\n",
    "        return pandas.Series({\"definiteArticlesNo\": definiteArticlesNo})\n",
    "\n",
    "    def getFirstPersonPronounsCounter(counter):\n",
    "        l = [k for k in counter.keys() if k.lower() in FirstPersonPronouns]\n",
    "        rs = {}\n",
    "        for k in l:\n",
    "            rs[k] = counter[k]\n",
    "        firstPersonPronouns = Counter(rs)\n",
    "        firstPersonPronounsNo = np.sum(firstPersonPronouns.values())\n",
    "        return pandas.Series(\n",
    "            {\n",
    "                \"firstPersonPronouns\": firstPersonPronouns,\n",
    "                \"firstPersonPronounsNo\": firstPersonPronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df[\"documentSimilarity\"] = df[\"text\"].apply(getDocumentSimilarity)\n",
    "    df[\"documentSimilarityNounsOrPronouns\"] = df[\"text\"].apply(\n",
    "        getDocumentSimilarity, args=(True,)\n",
    "    )\n",
    "    df[[\"pronouns\", \"pronounsNo\", \"sentencesNo\"]] = df[\"text\"].apply(getPronounsCounter)\n",
    "    df[[\"definiteArticlesNo\"]] = df[\"text\"].apply(getDefiniteArticlesCounter)\n",
    "    df[[\"firstPersonPronouns\", \"firstPersonPronounsNo\"]] = df[\"pronouns\"].apply(\n",
    "        getFirstPersonPronounsCounter\n",
    "    )\n",
    "    df[\"firstPersonPronounsRatio\"] = df[\"firstPersonPronounsNo\"] / df[\n",
    "        \"pronounsNo\"\n",
    "    ].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# def getIndications(df):\n",
    "# \tindications = pd.read_csv(\"meddra_all_label_indications.tsv\", delimiter='\\t', header=None)\n",
    "# \tindications = set(indications[8].tolist())\n",
    "# \tindications = \" \".join(indications)\n",
    "# \tdef textsOverlap(text, indications):\n",
    "# \t\trs = False\n",
    "# \t\ttokens=  text.text.split(\" \")\n",
    "# \t\tfor token in tokens:\n",
    "# \t\t\tif (\" \" + token + \" \") in indications:\n",
    "# \t\t\t\treturn True\n",
    "# \t\treturn False\n",
    "\n",
    "# \tfrom collections import Counter\n",
    "# \tdf['text'] = df.apply(getTextFromRecord,axis=1)\n",
    "# \trs = Counter()\n",
    "# \tfor row in df[['text']].itertuples():\n",
    "# \t\ttext = row[1]\n",
    "# \t\tdoc = nlp(unicode(text))\n",
    "# \t\tfor sentence in list(doc.sents):\n",
    "# \t\t\tnouns = [t for t in sentence  if t.pos_=='NOUN']\n",
    "# \t\t\trs += Counter([t.text.lower() for t in nouns if textsOverlap(t, indications)])\n",
    "# \t\t# rs += Counter([t.text.lower() for t in list(doc.noun_chunks) if textsOverlap(t, indications)])\n",
    "# \treturn rs\n",
    "\n",
    "\n",
    "def getSyntacticFeatures(row):\n",
    "    # http://stackoverflow.com/questions/33289820/noun-phrases-with-spacy\n",
    "    text = getTextFromRecord(row)\n",
    "\n",
    "    def getHeightToken(token):\n",
    "        height = 1\n",
    "        while token != token.head:\n",
    "            height += 1\n",
    "            token = token.head\n",
    "        return height\n",
    "\n",
    "    def getVerbPhrasesLength(sentence):\n",
    "        rs = [0]\n",
    "        inVerb = False\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if not inVerb:\n",
    "                    i = 1\n",
    "                inVerb = True\n",
    "                i += 1\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if inVerb:\n",
    "                    rs.append(i - 1)\n",
    "                inVerb = False\n",
    "        return rs\n",
    "\n",
    "    if text is None:\n",
    "        return pandas.Series(\n",
    "            {\n",
    "                \"maxHeight\": np.nan,\n",
    "                \"noun_chunks\": np.nan,\n",
    "                \"maxVerbPhraseLength\": np.nan,\n",
    "                \"subordinateConjuctions\": np.nan,\n",
    "            },\n",
    "            dtype=object,\n",
    "        )\n",
    "    doc = nlp(unicode(text))\n",
    "    noun_chunks = len(list(doc.noun_chunks))\n",
    "    sentences = list(doc.sents)\n",
    "    maxHeight = 1\n",
    "\n",
    "    subordinateConjuctions = 0\n",
    "    for sentence in sentences:\n",
    "        subordinateConjuctions += len(\n",
    "            [token for token in sentence if token.tag_ == \"IN\"]\n",
    "        )\n",
    "    subordinateConjuctions = subordinateConjuctions / float(len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        height = 1\n",
    "        if sentence.end == 0:\n",
    "            continue\n",
    "        sentenceHeight = max([getHeightToken(token) for token in sentence])\n",
    "        if maxHeight < sentenceHeight:\n",
    "            maxHeight = sentenceHeight\n",
    "\n",
    "    maxVerbPhraseLength = max(\n",
    "        max([getVerbPhrasesLength(sentence) for sentence in sentences])\n",
    "    )\n",
    "    return pandas.Series(\n",
    "        {\n",
    "            \"maxHeight\": maxHeight - 1,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"maxVerbPhraseLength\": maxVerbPhraseLength,\n",
    "            \"subordinateConjuctions\": subordinateConjuctions,\n",
    "        },\n",
    "        dtype=object,\n",
    "    )\n",
    "\n",
    "\n",
    "def getLanguageFeatures(df):\n",
    "    from collections import Counter\n",
    "    import textblob\n",
    "    import readability\n",
    "\n",
    "    comments = df[df[\"parent_id\"].astype(str) != \"nan\"]\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    rs = Counter()\n",
    "    # sentenceCount = 0\n",
    "    # wordsCount = 0\n",
    "    # wordsCounter = Counter()\n",
    "    for t in comments[[\"body\"]].itertuples():\n",
    "        o = textblob.TextBlob(t[1])\n",
    "        rs = rs + Counter([t[1] for t in o.pos_tags])\n",
    "        # sentenceCount = sentenceCount + len(o.raw_sentences)\n",
    "    for t in posts[[\"selftext\"]].itertuples():\n",
    "        o = textblob.TextBlob(t[1])\n",
    "        rs = rs + Counter([t[1] for t in o.pos_tags])\n",
    "        # sentenceCount = sentenceCount + len(o.raw_sentences)\n",
    "\n",
    "    # rs[wordsCounter] = np.mean(wordsCounter.values())/float(np.sum(wordsCounter.values()))\n",
    "    # rs['numSentences'] = sentenceCount/float(len(df))\n",
    "\n",
    "    total = float(np.sum(rs.values()))\n",
    "    rs1 = {k: (100 * rs[k]) / total for k in rs.keys()}\n",
    "    rs = pandas.DataFrame(rs1.items())\n",
    "    rs.columns = [\"PoS\", \"frequency\"]\n",
    "    rs = rs.sort(\"frequency\", ascending=False)\n",
    "    rs = rs[rs[\"PoS\"].isin(PoS)]\n",
    "    d = rs.set_index(\"PoS\")[\"frequency\"].to_dict()\n",
    "    d1 = {\"PoS-\" + k: d[k] for k in d.keys()}\n",
    "\n",
    "    df = readability.prepare(df)\n",
    "    df = readability.updateVocabularyFeatures(df)\n",
    "    df1[\"ll\"] = df[\"ll\"].mean()\n",
    "    return d1\n",
    "\n",
    "\n",
    "def addSyntacticFeatures(df):\n",
    "    df[\n",
    "        [\"maxHeight\", \"noun_chunks\", \"maxVerbPhraseLength\", \"subordinateConjuctions\"]\n",
    "    ] = df.apply(getSyntacticFeatures, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getURLtoPostRatio(df):\n",
    "    def containedInURL(row):\n",
    "        return row[\"permalink\"] not in row[\"url\"]\n",
    "\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    cnt = len(posts)\n",
    "    urls = len(posts[posts.apply(containedInURL, axis=1)])\n",
    "    return urls / float(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(df1, df2):\n",
    "    import readability\n",
    "\n",
    "    df1 = readability.prepare(df1)\n",
    "    df2 = readability.prepare(df2)\n",
    "\n",
    "    if \"text\" not in df1.columns:\n",
    "        df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    if \"text\" not in df2.columns:\n",
    "        df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df1[\"class\"] = \"class1\"\n",
    "    df2[\"class\"] = \"class2\"\n",
    "\n",
    "    df1 = df1[[\"text\", \"class\"]]\n",
    "    df2 = df2[[\"text\", \"class\"]]\n",
    "    data = pandas.concat([df1, df2])\n",
    "    data = data.reset_index()\n",
    "    del data[\"index\"]\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "\n",
    "def bagOfWords(ngram_range=(2, 2)):\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"count_vectorizer\",\n",
    "                CountVectorizer(ngram_range=ngram_range, analyzer=\"word\"),\n",
    "            ),\n",
    "            (\"classifier\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(pipeline, data=None, getScore=False):\n",
    "    k_fold = KFold(n=len(data), n_folds=6)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        train_text = data.iloc[train_indices][\"text\"].values\n",
    "        train_y = data.iloc[train_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices][\"text\"].values\n",
    "        test_y = data.iloc[test_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=\"class1\")\n",
    "        precision += precision_score(test_y, predictions, pos_label=\"class1\")\n",
    "        recall += recall_score(test_y, predictions, pos_label=\"class1\")\n",
    "        scores.append(score)\n",
    "\n",
    "    print(\"Total documents classified:\", len(data))\n",
    "    print(\"Score:\", sum(scores) / len(scores))\n",
    "    print(\"Precision:\", precision / len(scores))\n",
    "    print(\"Recall:\", recall / len(scores))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion)\n",
    "    if getScore:\n",
    "        return pipeline, sum(scores) / float(len(scores))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print \"\"\"USAGE:\n",
    "# get data as:\n",
    "# [['text', 'class']]\n",
    "\n",
    "# pipeline = bagOfWords()\n",
    "# pipeline = evaluate(pipeline, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binaryClassification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AD: https://github.com/imankulov/pickle-compat is an example of a compatibility layer.\n",
    "# in command lineline:\n",
    "# python -m pickle_compat.patch()\n",
    "# pickle.load suicidewatch-sample.pickle > data.tmp\n",
    "# python -m pickle.dump data.tmp > new_suicidewatch-sample.pickle -p pickle.HIGHEST_PROTOCOL\n",
    "# Select the lowest common protocols from py2 to py3: https://docs.python.org/3/library/pickle.html#data-stream-format\n",
    "\n",
    "subreddits = [\"suicidewatch-sample\", \"depression-sample\"]\n",
    "subreddits.sort()\n",
    "\n",
    "with open(\"suicidewatch-sample.pkl\", \"rb\") as f:\n",
    "    # Load the object from the file\n",
    "\n",
    "    object = pickle.load(f)\n",
    "\n",
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing depression-sample - suicidewatch-sample\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'depression-sample.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoing\u001b[39m\u001b[38;5;124m\"\u001b[39m, combination[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, combination[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 18\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     20\u001b[0m df1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mapply(getTextFromRecord, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adamd\\anaconda3\\envs\\bd_mha\\lib\\site-packages\\pandas\\io\\pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adamd\\anaconda3\\envs\\bd_mha\\lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'depression-sample.pickle'"
     ]
    }
   ],
   "source": [
    "fname = \"combinations-10fold.pickle\"\n",
    "combinations = list(itertools.combinations(subreddits, 2))\n",
    "\n",
    "for combination in combinations:\n",
    "    if os.path.isfile(fname):\n",
    "        f = open(fname, \"r\")\n",
    "        rs = pickle.load(f)\n",
    "        f.close()\n",
    "    else:\n",
    "        rs = {}\n",
    "\n",
    "    if combination[0] in rs.keys():\n",
    "        if combination[1] in rs[combination[0]].keys():\n",
    "            print(combination, \"already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "    print(\"doing\", combination[0], \"-\", combination[1])\n",
    "    df1 = pandas.read_pickle(combination[0] + \".pickle\")\n",
    "    df1 = df1.reset_index()\n",
    "    df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    df1 = df1.reindex(np.random.permutation(df1.index))\n",
    "    df2 = pandas.read_pickle(combination[1] + \".pickle\")\n",
    "    df2 = df2.reset_index()\n",
    "    df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df2 = df2.reindex(np.random.permutation(df2.index))\n",
    "    # keep only posts, keep only the text column\n",
    "    df1 = df1[df1[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df1 = df1.dropna(subset=[\"text\"])\n",
    "    df2 = df2[df2[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df2 = df2.dropna(subset=[\"text\"])\n",
    "\n",
    "    results = []\n",
    "    print(\"choosing min from\", len(df1), len(df2))\n",
    "    m = min(len(df1), len(df2))\n",
    "    for i in range(0, 10):\n",
    "        df1_min = df1.reindex(np.random.permutation(df1.index)).head(m)\n",
    "        df2_min = df2.reindex(np.random.permutation(df2.index)).head(m)\n",
    "\n",
    "        data = ml.getData(df1_min, df2_min)\n",
    "        print(\"got\", len(data), \"records...training\")\n",
    "        pipeline = bagOfWords(ngram_range=ngram_range)\n",
    "        pipeline, score = evaluate(pipeline, data=data, getScore=True)\n",
    "        results.append(score)\n",
    "\n",
    "    print(\"RESULTS:\", combination, results)\n",
    "    if combination[0] not in rs.keys():\n",
    "        rs[combination[0]] = {}\n",
    "    rs[combination[0]][combination[1]] = results\n",
    "\n",
    "    f = open(fname, \"w\")\n",
    "    pickle.dump(rs, f)\n",
    "    f.close()\n",
    "    print(\"finished\", combination)\n",
    "\n",
    "\n",
    "def readResults(deviations=False):\n",
    "    df = pandas.read_pickle(\"combinations-10fold.pickle\")\n",
    "    df = pandas.DataFrame(df)\n",
    "    for column in df.columns:\n",
    "        for index in df.index:\n",
    "            tmp = df[column][index]\n",
    "            if tmp is None:\n",
    "                continue\n",
    "            if deviations:\n",
    "                df[column][index] = np.std(tmp)\n",
    "            else:\n",
    "                df[column][index] = np.mean(tmp)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
