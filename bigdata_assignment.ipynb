{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install readability-lxml\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from readability import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load EN from None\n",
      "doing ./data/depression-sample - ./data/suicidewatch-sample\n",
      "choosing min from 857 555\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6666033350411994\n",
      "\n",
      "Precision: 0.656450902569578\n",
      "\n",
      "Recall: 0.6815438751526721\n",
      "\n",
      "Confusion matrix:\n",
      "[[378 177]\n",
      " [200 355]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6387828445939984\n",
      "\n",
      "Precision: 0.7090247101204192\n",
      "\n",
      "Recall: 0.6045402386023842\n",
      "\n",
      "Confusion matrix:\n",
      "[[331 224]\n",
      " [149 406]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.651782599662758\n",
      "\n",
      "Precision: 0.6586455003331314\n",
      "\n",
      "Recall: 0.6522876565584951\n",
      "\n",
      "Confusion matrix:\n",
      "[[361 194]\n",
      " [190 365]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6468897765818583\n",
      "\n",
      "Precision: 0.6856096438271077\n",
      "\n",
      "Recall: 0.6251648795116357\n",
      "\n",
      "Confusion matrix:\n",
      "[[346 209]\n",
      " [168 387]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6201117605077056\n",
      "\n",
      "Precision: 0.6828630668835958\n",
      "\n",
      "Recall: 0.5940381459111274\n",
      "\n",
      "Confusion matrix:\n",
      "[[324 231]\n",
      " [163 392]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.642217035938795\n",
      "\n",
      "Precision: 0.6692168345781481\n",
      "\n",
      "Recall: 0.62809181787483\n",
      "\n",
      "Confusion matrix:\n",
      "[[347 208]\n",
      " [177 378]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6558053133267666\n",
      "\n",
      "Precision: 0.6991320950758916\n",
      "\n",
      "Recall: 0.6262041018369823\n",
      "\n",
      "Confusion matrix:\n",
      "[[346 209]\n",
      " [154 401]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6389918658504082\n",
      "\n",
      "Precision: 0.6555320606635188\n",
      "\n",
      "Recall: 0.6314652142802993\n",
      "\n",
      "Confusion matrix:\n",
      "[[349 206]\n",
      " [188 367]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6654569433297469\n",
      "\n",
      "Precision: 0.6883481927647019\n",
      "\n",
      "Recall: 0.6554614929179245\n",
      "\n",
      "Confusion matrix:\n",
      "[[361 194]\n",
      " [169 386]]\n",
      "got 1110 records...training\n",
      "\n",
      "Total documents classified: 1110\n",
      "\n",
      "Score: 0.6658870312016262\n",
      "\n",
      "Precision: 0.6975074189516838\n",
      "\n",
      "Recall: 0.6460324223435977\n",
      "\n",
      "Confusion matrix:\n",
      "[[357 198]\n",
      " [159 396]]\n",
      "RESULTS: ('./data/depression-sample', './data/suicidewatch-sample') [0.6666033350411994, 0.6387828445939984, 0.651782599662758, 0.6468897765818583, 0.6201117605077056, 0.642217035938795, 0.6558053133267666, 0.6389918658504082, 0.6654569433297469, 0.6658870312016262]\n",
      "finished ('./data/depression-sample', './data/suicidewatch-sample')\n"
     ]
    }
   ],
   "source": [
    "PoS = [\"NN\", \"PRP\", \"IN\", \"DT\", \"RB\", \"JJ\", \"VB\", \"CC\", \"NNS\", \"VBP\"]\n",
    "FirstPersonPronouns = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"mine\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"us\",\n",
    "    \"we\",\n",
    "]\n",
    "\n",
    "if \"SPACY_DATA\" in os.environ:\n",
    "    data_dir = os.environ[\"SPACY_DATA\"]\n",
    "else:\n",
    "    data_dir = None\n",
    "print(\"Load EN from %s\" % data_dir)\n",
    "\n",
    "nlp = English(data_dir=data_dir)\n",
    "\n",
    "\n",
    "def getTextFromRecord(row):\n",
    "    if not pd.isnull(row[\"body\"]):\n",
    "        rs = row[\"body\"]\n",
    "    else:\n",
    "        rs = row[\"selftext\"]\n",
    "    if rs == \"[deleted]\":\n",
    "        return None\n",
    "    return rs\n",
    "\n",
    "\n",
    "def addLexicalFeatures(df):\n",
    "    from collections import Counter\n",
    "\n",
    "    df[\"text\"] = df.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    def cosineSimilarity(sentence1, sentence2, NounsOrPronounsOnly=False):\n",
    "        import math\n",
    "\n",
    "        if not NounsOrPronounsOnly:\n",
    "            vector1 = Counter([token.text.lower() for token in sentence1])\n",
    "            vector2 = Counter([token.text.lower() for token in sentence2])\n",
    "        else:\n",
    "            vector1 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence1\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "            vector2 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence2\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "        intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "        numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
    "        sum1 = sum([vector1[x] ** 2 for x in vector1.keys()])\n",
    "        sum2 = sum([vector2[x] ** 2 for x in vector2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "\n",
    "    def getDocumentSimilarity(text, NounsOrPronounsOnly=False):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "        rs = 0\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            rs += cosineSimilarity(sentences[i], sentences[i + 1], NounsOrPronounsOnly)\n",
    "        rs = rs / float(len(sentences) - 1)\n",
    "        return rs\n",
    "\n",
    "    def getPronounsCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        pronouns = []\n",
    "        for sentence in sentences:\n",
    "            pronouns.extend(\n",
    "                [token.text for token in sentence if token.tag_.startswith(\"PRP\")]\n",
    "            )\n",
    "\n",
    "        pronouns = Counter(pronouns)\n",
    "        pronounsNo = np.sum(list(pronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"sentencesNo\": len(sentences),\n",
    "                \"pronouns\": pronouns,\n",
    "                \"pronounsNo\": pronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def getDefiniteArticlesCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        definiteArticles = []\n",
    "        for sentence in sentences:\n",
    "            definiteArticles.extend(\n",
    "                [\n",
    "                    token.text\n",
    "                    for token in sentence\n",
    "                    if (token.tag_ == \"DT\" and token.text.lower() == \"the\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        definiteArticles = Counter(definiteArticles)\n",
    "        definiteArticlesNo = np.sum(list(definiteArticles.values()))\n",
    "        return pd.Series({\"definiteArticlesNo\": definiteArticlesNo})\n",
    "\n",
    "    def getFirstPersonPronounsCounter(counter):\n",
    "        l = [k for k in counter.keys() if k.lower() in FirstPersonPronouns]\n",
    "        rs = {}\n",
    "        for k in l:\n",
    "            rs[k] = counter[k]\n",
    "        firstPersonPronouns = Counter(rs)\n",
    "        firstPersonPronounsNo = np.sum(list(firstPersonPronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"firstPersonPronouns\": firstPersonPronouns,\n",
    "                \"firstPersonPronounsNo\": firstPersonPronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df[\"documentSimilarity\"] = df[\"text\"].apply(getDocumentSimilarity)\n",
    "    df[\"documentSimilarityNounsOrPronouns\"] = df[\"text\"].apply(\n",
    "        getDocumentSimilarity, args=(True,)\n",
    "    )\n",
    "    df[[\"pronouns\", \"pronounsNo\", \"sentencesNo\"]] = df[\"text\"].apply(getPronounsCounter)\n",
    "    df[[\"definiteArticlesNo\"]] = df[\"text\"].apply(getDefiniteArticlesCounter)\n",
    "    df[[\"firstPersonPronouns\", \"firstPersonPronounsNo\"]] = df[\"pronouns\"].apply(\n",
    "        getFirstPersonPronounsCounter\n",
    "    )\n",
    "    df[\"firstPersonPronounsRatio\"] = df[\"firstPersonPronounsNo\"] / df[\n",
    "        \"pronounsNo\"\n",
    "    ].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getSyntacticFeatures(row):\n",
    "    text = getTextFromRecord(row)\n",
    "\n",
    "    def getHeightToken(token):\n",
    "        height = 1\n",
    "        while token != token.head:\n",
    "            height += 1\n",
    "            token = token.head\n",
    "        return height\n",
    "\n",
    "    def getVerbPhrasesLength(sentence):\n",
    "        rs = [0]\n",
    "        inVerb = False\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if not inVerb:\n",
    "                    i = 1\n",
    "                inVerb = True\n",
    "                i += 1\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if inVerb:\n",
    "                    rs.append(i - 1)\n",
    "                inVerb = False\n",
    "        return rs\n",
    "\n",
    "    if text is None:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"maxHeight\": np.nan,\n",
    "                \"noun_chunks\": np.nan,\n",
    "                \"maxVerbPhraseLength\": np.nan,\n",
    "                \"subordinateConjuctions\": np.nan,\n",
    "            },\n",
    "            dtype=object,\n",
    "        )\n",
    "    doc = nlp(str(text))\n",
    "    noun_chunks = len(list(doc.noun_chunks))\n",
    "    sentences = list(doc.sents)\n",
    "    maxHeight = 1\n",
    "\n",
    "    subordinateConjuctions = 0\n",
    "    for sentence in sentences:\n",
    "        subordinateConjuctions += len(\n",
    "            [token for token in sentence if token.tag_ == \"IN\"]\n",
    "        )\n",
    "    subordinateConjuctions = subordinateConjuctions / float(len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        height = 1\n",
    "        if sentence.end == 0:\n",
    "            continue\n",
    "        sentenceHeight = max([getHeightToken(token) for token in sentence])\n",
    "        if maxHeight < sentenceHeight:\n",
    "            maxHeight = sentenceHeight\n",
    "\n",
    "    maxVerbPhraseLength = max(\n",
    "        max([getVerbPhrasesLength(sentence) for sentence in sentences])\n",
    "    )\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"maxHeight\": maxHeight - 1,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"maxVerbPhraseLength\": maxVerbPhraseLength,\n",
    "            \"subordinateConjuctions\": subordinateConjuctions,\n",
    "        },\n",
    "        dtype=object,\n",
    "    )\n",
    "\n",
    "\n",
    "def getURLtoPostRatio(df):\n",
    "    def containedInURL(row):\n",
    "        return row[\"permalink\"] not in row[\"url\"]\n",
    "\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    cnt = len(posts)\n",
    "    urls = len(posts[posts.apply(containedInURL, axis=1)])\n",
    "    return urls / float(cnt)\n",
    "\n",
    "\n",
    "# def getData(df1, df2):\n",
    "#     import readability\n",
    "#     df1 = readability.prepare(df1)\n",
    "#     df2 = readability.prepare(df2)\n",
    "\n",
    "#     if \"text\" not in df1.columns:\n",
    "#         df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "#     if \"text\" not in df2.columns:\n",
    "#         df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "#     df1[\"class\"] = \"class1\"\n",
    "#     df2[\"class\"] = \"class2\"\n",
    "\n",
    "#     df1 = df1[[\"text\", \"class\"]]\n",
    "#     df2 = df2[[\"text\", \"class\"]]\n",
    "#     data = pd.concat([df1, df2])\n",
    "#     data = data.reset_index()\n",
    "#     del data[\"index\"]\n",
    "#     data = data.reindex(np.random.permutation(data.index))\n",
    "#     return data\n",
    "\n",
    "\n",
    "def preprocess_record(record):\n",
    "    doc = Document(record)\n",
    "    return doc.summary()\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df['text'] = df['text'].apply(preprocess_record)\n",
    "    return df\n",
    "\n",
    "def getData(df1, df2):\n",
    "    df1_processed = preprocess_dataframe(df1)\n",
    "    df2_processed = preprocess_dataframe(df2)\n",
    "\n",
    "    if \"text\" not in df1_processed.columns:\n",
    "        df1_processed[\"text\"] = df1_processed.apply(getTextFromRecord, axis=1)\n",
    "    if \"text\" not in df2_processed.columns:\n",
    "        df2_processed[\"text\"] = df2_processed.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    df1_processed[\"class\"] = \"class1\"\n",
    "    df2_processed[\"class\"] = \"class2\"\n",
    "\n",
    "    df1_processed = df1_processed[[\"text\", \"class\"]]\n",
    "    df2_processed = df2_processed[[\"text\", \"class\"]]\n",
    "\n",
    "    data = pd.concat([df1_processed, df2_processed])\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bagOfWords(ngram_range=(2, 2)):\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"count_vectorizer\",\n",
    "                CountVectorizer(ngram_range=ngram_range, analyzer=\"word\"),\n",
    "            ),\n",
    "            (\"classifier\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(pipeline, data, getScore=False):\n",
    "    k_fold = KFold(n_splits=6)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for train_indices, test_indices in k_fold.split(data):\n",
    "        train_text = data.iloc[train_indices][\"text\"].values\n",
    "        train_y = data.iloc[train_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices][\"text\"].values\n",
    "        test_y = data.iloc[test_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=\"class1\")\n",
    "        precision += precision_score(test_y, predictions, pos_label=\"class1\")\n",
    "        recall += recall_score(test_y, predictions, pos_label=\"class1\")\n",
    "        scores.append(score)\n",
    "\n",
    "    print(\"\\nTotal documents classified:\", len(data))\n",
    "    print(\"\\nScore:\", sum(scores) / len(scores))\n",
    "    print(\"\\nPrecision:\", precision / len(scores))\n",
    "    print(\"\\nRecall:\", recall / len(scores))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion)\n",
    "    if getScore:\n",
    "        return pipeline, sum(scores) / float(len(scores))\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def readResults(deviations=False):\n",
    "    df = pd.read_pickle(\"./data/combinations-10fold.pkl\")\n",
    "    df = pd.DataFrame(df)\n",
    "    for column in df.columns:\n",
    "        for index in df.index:\n",
    "            tmp = df[column][index]\n",
    "            if tmp is None:\n",
    "                continue\n",
    "            if deviations:\n",
    "                df[column][index] = np.std(tmp)\n",
    "            else:\n",
    "                df[column][index] = np.mean(tmp)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#binaryClassification.py\n",
    "\n",
    "subreddits = [\"./data/suicidewatch-sample\", \"./data/depression-sample\"]\n",
    "subreddits.sort()\n",
    "\n",
    "\n",
    "ngram_range = (1, 2)\n",
    "fname = \"./data/combinations-10fold.pkl\"\n",
    "combinations = list(itertools.combinations(subreddits, 2))\n",
    "\n",
    "for combination in combinations:\n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname, \"rb\") as f:\n",
    "            rs = pickle.load(f)\n",
    "    else:\n",
    "        rs = {}\n",
    "\n",
    "    # if combination[0] in rs.keys():\n",
    "    #     if combination[1] in rs[combination[0]].keys():\n",
    "    #         print(combination, \"already exists, skipping...\")\n",
    "    #         continue\n",
    "\n",
    "    print(\"doing\", combination[0], \"-\", combination[1])\n",
    "    df1 = pd.read_pickle(combination[0] + \".pkl\")\n",
    "    df1 = df1.reset_index()\n",
    "    df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    df1 = df1.reindex(np.random.permutation(df1.index))\n",
    "    df2 = pd.read_pickle(combination[1] + \".pkl\")\n",
    "    df2 = df2.reset_index()\n",
    "    df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df2 = df2.reindex(np.random.permutation(df2.index))\n",
    "\n",
    "    # keep only posts, keep only the text column\n",
    "    df1 = df1[df1[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df1 = df1.dropna(subset=[\"text\"])\n",
    "    df2 = df2[df2[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df2 = df2.dropna(subset=[\"text\"])\n",
    "\n",
    "\n",
    "    results = []\n",
    "    print(\"choosing min from\", len(df1), len(df2))\n",
    "    m = min(len(df1), len(df2))\n",
    "    for i in range(0, 10):\n",
    "        df1_min = df1.reindex(np.random.permutation(df1.index)).head(m)\n",
    "        df2_min = df2.reindex(np.random.permutation(df2.index)).head(m)\n",
    "\n",
    "        data = getData(df1_min, df2_min)\n",
    "        print(\"got\", len(data), \"records...training\")\n",
    "        pipeline = bagOfWords(ngram_range=ngram_range)\n",
    "        pipeline, score = evaluate(pipeline, data=data, getScore=True)\n",
    "        results.append(score)\n",
    "\n",
    "print(\"RESULTS:\", combination, results)\n",
    "if combination[0] not in rs.keys():\n",
    "    rs[combination[0]] = {}\n",
    "    rs[combination[0]][combination[1]] = results\n",
    "\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump(rs, f)\n",
    "    print(\"finished\", combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reference to BagOfWords:\n",
    "\n",
    "Character n-gram Language Models Language models are frequently employed to estimate the likelihood of a given sequence of words. This is done often by examining a\n",
    "moving window of n words (n-gram). Traditionally each word is treated as a token, but\n",
    "previous work indicates that treating each character as a token creates classifiers that capture some of the creative language use and emoticons frequently found on social media and\n",
    "are afforded a modicum of robustness to misspellings [McNamee and Mayfield, 2004,Coppersmith et al., 2015a].\n",
    "\n",
    "https://qntfy.com/static/papers/jsm2015.pdf\n",
    "\n",
    "\n",
    "In this example we are using words as a token set in Pipeline: </br>\n",
    "CountVectorizer - converts collection of text docs to a matrix of tokens counts - using the specified ngramrange </br>\n",
    "Analyzer - set to \"word\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
