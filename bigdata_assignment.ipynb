{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting readability-lxml\n",
      "  Downloading readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: chardet in c:\\users\\spichr01\\appdata\\local\\anaconda3\\envs\\bigdataassignment\\lib\\site-packages (from readability-lxml) (5.2.0)\n",
      "Collecting lxml (from readability-lxml)\n",
      "  Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting cssselect (from readability-lxml)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.7/3.8 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/3.8 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.8 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.7/3.8 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.4/3.8 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 12.1 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml, cssselect, readability-lxml\n",
      "Successfully installed cssselect-1.2.0 lxml-4.9.3 readability-lxml-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install readability-lxml\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load EN from None\n",
      "Load EN from None\n",
      "('./data/depression-sample', './data/suicidewatch-sample') already exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "PoS = [\"NN\", \"PRP\", \"IN\", \"DT\", \"RB\", \"JJ\", \"VB\", \"CC\", \"NNS\", \"VBP\"]\n",
    "FirstPersonPronouns = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"mine\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"us\",\n",
    "    \"we\",\n",
    "]\n",
    "\n",
    "if \"SPACY_DATA\" in os.environ:\n",
    "    data_dir = os.environ[\"SPACY_DATA\"]\n",
    "else:\n",
    "    data_dir = None\n",
    "print(\"Load EN from %s\" % data_dir)\n",
    "\n",
    "nlp = English(data_dir=data_dir)\n",
    "\n",
    "\n",
    "def getTextFromRecord(row):\n",
    "    if not pd.isnull(row[\"body\"]):\n",
    "        rs = row[\"body\"]\n",
    "    else:\n",
    "        rs = row[\"selftext\"]\n",
    "    if rs == \"[deleted]\":\n",
    "        return None\n",
    "    return rs\n",
    "\n",
    "\n",
    "def addLexicalFeatures(df):\n",
    "    from collections import Counter\n",
    "\n",
    "    df[\"text\"] = df.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    def cosineSimilarity(sentence1, sentence2, NounsOrPronounsOnly=False):\n",
    "        import math\n",
    "\n",
    "        if not NounsOrPronounsOnly:\n",
    "            vector1 = Counter([token.text.lower() for token in sentence1])\n",
    "            vector2 = Counter([token.text.lower() for token in sentence2])\n",
    "        else:\n",
    "            vector1 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence1\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "            vector2 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence2\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "        intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "        numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
    "        sum1 = sum([vector1[x] ** 2 for x in vector1.keys()])\n",
    "        sum2 = sum([vector2[x] ** 2 for x in vector2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "\n",
    "    def getDocumentSimilarity(text, NounsOrPronounsOnly=False):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "        rs = 0\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            rs += cosineSimilarity(sentences[i], sentences[i + 1], NounsOrPronounsOnly)\n",
    "        rs = rs / float(len(sentences) - 1)\n",
    "        return rs\n",
    "\n",
    "    def getPronounsCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        pronouns = []\n",
    "        for sentence in sentences:\n",
    "            pronouns.extend(\n",
    "                [token.text for token in sentence if token.tag_.startswith(\"PRP\")]\n",
    "            )\n",
    "\n",
    "        pronouns = Counter(pronouns)\n",
    "        pronounsNo = np.sum(list(pronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"sentencesNo\": len(sentences),\n",
    "                \"pronouns\": pronouns,\n",
    "                \"pronounsNo\": pronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def getDefiniteArticlesCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        definiteArticles = []\n",
    "        for sentence in sentences:\n",
    "            definiteArticles.extend(\n",
    "                [\n",
    "                    token.text\n",
    "                    for token in sentence\n",
    "                    if (token.tag_ == \"DT\" and token.text.lower() == \"the\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        definiteArticles = Counter(definiteArticles)\n",
    "        definiteArticlesNo = np.sum(list(definiteArticles.values()))\n",
    "        return pd.Series({\"definiteArticlesNo\": definiteArticlesNo})\n",
    "\n",
    "    def getFirstPersonPronounsCounter(counter):\n",
    "        l = [k for k in counter.keys() if k.lower() in FirstPersonPronouns]\n",
    "        rs = {}\n",
    "        for k in l:\n",
    "            rs[k] = counter[k]\n",
    "        firstPersonPronouns = Counter(rs)\n",
    "        firstPersonPronounsNo = np.sum(list(firstPersonPronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"firstPersonPronouns\": firstPersonPronouns,\n",
    "                \"firstPersonPronounsNo\": firstPersonPronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df[\"documentSimilarity\"] = df[\"text\"].apply(getDocumentSimilarity)\n",
    "    df[\"documentSimilarityNounsOrPronouns\"] = df[\"text\"].apply(\n",
    "        getDocumentSimilarity, args=(True,)\n",
    "    )\n",
    "    df[[\"pronouns\", \"pronounsNo\", \"sentencesNo\"]] = df[\"text\"].apply(getPronounsCounter)\n",
    "    df[[\"definiteArticlesNo\"]] = df[\"text\"].apply(getDefiniteArticlesCounter)\n",
    "    df[[\"firstPersonPronouns\", \"firstPersonPronounsNo\"]] = df[\"pronouns\"].apply(\n",
    "        getFirstPersonPronounsCounter\n",
    "    )\n",
    "    df[\"firstPersonPronounsRatio\"] = df[\"firstPersonPronounsNo\"] / df[\n",
    "        \"pronounsNo\"\n",
    "    ].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getSyntacticFeatures(row):\n",
    "    text = getTextFromRecord(row)\n",
    "\n",
    "    def getHeightToken(token):\n",
    "        height = 1\n",
    "        while token != token.head:\n",
    "            height += 1\n",
    "            token = token.head\n",
    "        return height\n",
    "\n",
    "    def getVerbPhrasesLength(sentence):\n",
    "        rs = [0]\n",
    "        inVerb = False\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if not inVerb:\n",
    "                    i = 1\n",
    "                inVerb = True\n",
    "                i += 1\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if inVerb:\n",
    "                    rs.append(i - 1)\n",
    "                inVerb = False\n",
    "        return rs\n",
    "\n",
    "    if text is None:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"maxHeight\": np.nan,\n",
    "                \"noun_chunks\": np.nan,\n",
    "                \"maxVerbPhraseLength\": np.nan,\n",
    "                \"subordinateConjuctions\": np.nan,\n",
    "            },\n",
    "            dtype=object,\n",
    "        )\n",
    "    doc = nlp(str(text))\n",
    "    noun_chunks = len(list(doc.noun_chunks))\n",
    "    sentences = list(doc.sents)\n",
    "    maxHeight = 1\n",
    "\n",
    "    subordinateConjuctions = 0\n",
    "    for sentence in sentences:\n",
    "        subordinateConjuctions += len(\n",
    "            [token for token in sentence if token.tag_ == \"IN\"]\n",
    "        )\n",
    "    subordinateConjuctions = subordinateConjuctions / float(len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        height = 1\n",
    "        if sentence.end == 0:\n",
    "            continue\n",
    "        sentenceHeight = max([getHeightToken(token) for token in sentence])\n",
    "        if maxHeight < sentenceHeight:\n",
    "            maxHeight = sentenceHeight\n",
    "\n",
    "    maxVerbPhraseLength = max(\n",
    "        max([getVerbPhrasesLength(sentence) for sentence in sentences])\n",
    "    )\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"maxHeight\": maxHeight - 1,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"maxVerbPhraseLength\": maxVerbPhraseLength,\n",
    "            \"subordinateConjuctions\": subordinateConjuctions,\n",
    "        },\n",
    "        dtype=object,\n",
    "    )\n",
    "\n",
    "\n",
    "def getURLtoPostRatio(df):\n",
    "    def containedInURL(row):\n",
    "        return row[\"permalink\"] not in row[\"url\"]\n",
    "\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    cnt = len(posts)\n",
    "    urls = len(posts[posts.apply(containedInURL, axis=1)])\n",
    "    return urls / float(cnt)\n",
    "\n",
    "\n",
    "def getData(df1, df2):\n",
    "    import readability\n",
    "\n",
    "    df1 = readability.prepare(df1)\n",
    "    df2 = readability.prepare(df2)\n",
    "\n",
    "    if \"text\" not in df1.columns:\n",
    "        df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    if \"text\" not in df2.columns:\n",
    "        df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df1[\"class\"] = \"class1\"\n",
    "    df2[\"class\"] = \"class2\"\n",
    "\n",
    "    df1 = df1[[\"text\", \"class\"]]\n",
    "    df2 = df2[[\"text\", \"class\"]]\n",
    "    data = pd.concat([df1, df2])\n",
    "    data = data.reset_index()\n",
    "    del data[\"index\"]\n",
    "    data = data.reindex(np.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "\n",
    "def bagOfWords(ngram_range=(2, 2)):\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"count_vectorizer\",\n",
    "                CountVectorizer(ngram_range=ngram_range, analyzer=\"word\"),\n",
    "            ),\n",
    "            (\"classifier\", MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(pipeline, data, getScore=False):\n",
    "    k_fold = KFold(n_splits=6)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    for train_indices, test_indices in k_fold.split(data):\n",
    "        train_text = data.iloc[train_indices][\"text\"].values\n",
    "        train_y = data.iloc[train_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices][\"text\"].values\n",
    "        test_y = data.iloc[test_indices][\"class\"].values.astype(str)\n",
    "\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=\"class1\")\n",
    "        precision += precision_score(test_y, predictions, pos_label=\"class1\")\n",
    "        recall += recall_score(test_y, predictions, pos_label=\"class1\")\n",
    "        scores.append(score)\n",
    "\n",
    "    print(\"Total documents classified:\", len(data))\n",
    "    print(\"Score:\", sum(scores) / len(scores))\n",
    "    print(\"Precision:\", precision / len(scores))\n",
    "    print(\"Recall:\", recall / len(scores))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion)\n",
    "    if getScore:\n",
    "        return pipeline, sum(scores) / float(len(scores))\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "PoS = [\"NN\", \"PRP\", \"IN\", \"DT\", \"RB\", \"JJ\", \"VB\", \"CC\", \"NNS\", \"VBP\"]\n",
    "FirstPersonPronouns = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"mine\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"us\",\n",
    "    \"we\",\n",
    "]\n",
    "\n",
    "if \"SPACY_DATA\" in os.environ:\n",
    "    data_dir = os.environ[\"SPACY_DATA\"]\n",
    "else:\n",
    "    data_dir = None\n",
    "print(\"Load EN from %s\" % data_dir)\n",
    "\n",
    "nlp = English(data_dir=data_dir)\n",
    "\n",
    "\n",
    "def getTextFromRecord(row):\n",
    "    if not pd.isnull(row[\"body\"]):\n",
    "        rs = row[\"body\"]\n",
    "    else:\n",
    "        rs = row[\"selftext\"]\n",
    "    if rs == \"[deleted]\":\n",
    "        return None\n",
    "    return rs\n",
    "\n",
    "\n",
    "def addLexicalFeatures(df):\n",
    "    from collections import Counter\n",
    "\n",
    "    df[\"text\"] = df.apply(getTextFromRecord, axis=1)\n",
    "\n",
    "    def cosineSimilarity(sentence1, sentence2, NounsOrPronounsOnly=False):\n",
    "        import math\n",
    "\n",
    "        if not NounsOrPronounsOnly:\n",
    "            vector1 = Counter([token.text.lower() for token in sentence1])\n",
    "            vector2 = Counter([token.text.lower() for token in sentence2])\n",
    "        else:\n",
    "            vector1 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence1\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "            vector2 = Counter(\n",
    "                [\n",
    "                    token.text.lower()\n",
    "                    for token in sentence2\n",
    "                    if token.tag_.startswith(\"N\") or token.tag_.startswith(\"PR\")\n",
    "                ]\n",
    "            )\n",
    "        intersection = set(vector1.keys()) & set(vector2.keys())\n",
    "        numerator = sum([vector1[x] * vector2[x] for x in intersection])\n",
    "        sum1 = sum([vector1[x] ** 2 for x in vector1.keys()])\n",
    "        sum2 = sum([vector2[x] ** 2 for x in vector2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "\n",
    "    def getDocumentSimilarity(text, NounsOrPronounsOnly=False):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "        rs = 0\n",
    "        for i in range(0, len(sentences) - 1):\n",
    "            rs += cosineSimilarity(sentences[i], sentences[i + 1], NounsOrPronounsOnly)\n",
    "        rs = rs / float(len(sentences) - 1)\n",
    "        return rs\n",
    "\n",
    "    def getPronounsCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        pronouns = []\n",
    "        for sentence in sentences:\n",
    "            pronouns.extend(\n",
    "                [token.text for token in sentence if token.tag_.startswith(\"PRP\")]\n",
    "            )\n",
    "\n",
    "        pronouns = Counter(pronouns)\n",
    "        pronounsNo = np.sum(list(pronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"sentencesNo\": len(sentences),\n",
    "                \"pronouns\": pronouns,\n",
    "                \"pronounsNo\": pronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def getDefiniteArticlesCounter(text):\n",
    "        doc = nlp(str(text))\n",
    "        sentences = list(doc.sents)\n",
    "        from collections import Counter\n",
    "\n",
    "        definiteArticles = []\n",
    "        for sentence in sentences:\n",
    "            definiteArticles.extend(\n",
    "                [\n",
    "                    token.text\n",
    "                    for token in sentence\n",
    "                    if (token.tag_ == \"DT\" and token.text.lower() == \"the\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        definiteArticles = Counter(definiteArticles)\n",
    "        definiteArticlesNo = np.sum(list(definiteArticles.values()))\n",
    "        return pd.Series({\"definiteArticlesNo\": definiteArticlesNo})\n",
    "\n",
    "    def getFirstPersonPronounsCounter(counter):\n",
    "        l = [k for k in counter.keys() if k.lower() in FirstPersonPronouns]\n",
    "        rs = {}\n",
    "        for k in l:\n",
    "            rs[k] = counter[k]\n",
    "        firstPersonPronouns = Counter(rs)\n",
    "        firstPersonPronounsNo = np.sum(list(firstPersonPronouns.values()))\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"firstPersonPronouns\": firstPersonPronouns,\n",
    "                \"firstPersonPronounsNo\": firstPersonPronounsNo,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df[\"documentSimilarity\"] = df[\"text\"].apply(getDocumentSimilarity)\n",
    "    df[\"documentSimilarityNounsOrPronouns\"] = df[\"text\"].apply(\n",
    "        getDocumentSimilarity, args=(True,)\n",
    "    )\n",
    "    df[[\"pronouns\", \"pronounsNo\", \"sentencesNo\"]] = df[\"text\"].apply(getPronounsCounter)\n",
    "    df[[\"definiteArticlesNo\"]] = df[\"text\"].apply(getDefiniteArticlesCounter)\n",
    "    df[[\"firstPersonPronouns\", \"firstPersonPronounsNo\"]] = df[\"pronouns\"].apply(\n",
    "        getFirstPersonPronounsCounter\n",
    "    )\n",
    "    df[\"firstPersonPronounsRatio\"] = df[\"firstPersonPronounsNo\"] / df[\n",
    "        \"pronounsNo\"\n",
    "    ].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getSyntacticFeatures(row):\n",
    "    text = getTextFromRecord(row)\n",
    "\n",
    "    def getHeightToken(token):\n",
    "        height = 1\n",
    "        while token != token.head:\n",
    "            height += 1\n",
    "            token = token.head\n",
    "        return height\n",
    "\n",
    "    def getVerbPhrasesLength(sentence):\n",
    "        rs = [0]\n",
    "        inVerb = False\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if not inVerb:\n",
    "                    i = 1\n",
    "                inVerb = True\n",
    "                i += 1\n",
    "            if token.pos_ != \"VERB\":\n",
    "                if inVerb:\n",
    "                    rs.append(i - 1)\n",
    "                inVerb = False\n",
    "        return rs\n",
    "\n",
    "    if text is None:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"maxHeight\": np.nan,\n",
    "                \"noun_chunks\": np.nan,\n",
    "                \"maxVerbPhraseLength\": np.nan,\n",
    "                \"subordinateConjuctions\": np.nan,\n",
    "            },\n",
    "            dtype=object,\n",
    "        )\n",
    "    doc = nlp(str(text))\n",
    "    noun_chunks = len(list(doc.noun_chunks))\n",
    "    sentences = list(doc.sents)\n",
    "    maxHeight = 1\n",
    "\n",
    "    subordinateConjuctions = 0\n",
    "    for sentence in sentences:\n",
    "        subordinateConjuctions += len(\n",
    "            [token for token in sentence if token.tag_ == \"IN\"]\n",
    "        )\n",
    "    subordinateConjuctions = subordinateConjuctions / float(len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        height = 1\n",
    "        if sentence.end == 0:\n",
    "            continue\n",
    "        sentenceHeight = max([getHeightToken(token) for token in sentence])\n",
    "        if maxHeight < sentenceHeight:\n",
    "            maxHeight = sentenceHeight\n",
    "\n",
    "    maxVerbPhraseLength = max(\n",
    "        max([getVerbPhrasesLength(sentence) for sentence in sentences])\n",
    "    )\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"maxHeight\": maxHeight - 1,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"maxVerbPhraseLength\": maxVerbPhraseLength,\n",
    "            \"subordinateConjuctions\": subordinateConjuctions,\n",
    "        },\n",
    "        dtype=object,\n",
    "    )\n",
    "\n",
    "\n",
    "def getURLtoPostRatio(df):\n",
    "    def containedInURL(row):\n",
    "        return row[\"permalink\"] not in row[\"url\"]\n",
    "\n",
    "    posts = df[df[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    cnt = len(posts)\n",
    "    urls = len(posts[posts.apply(containedInURL, axis=1)])\n",
    "    return urls / float(cnt)\n",
    "\n",
    "\n",
    "def readResults(deviations=False):\n",
    "    df = pd.read_pickle(\"./data/combinations-10fold.pkl\")\n",
    "    df = pd.DataFrame(df)\n",
    "    for column in df.columns:\n",
    "        for index in df.index:\n",
    "            tmp = df[column][index]\n",
    "            if tmp is None:\n",
    "                continue\n",
    "            if deviations:\n",
    "                df[column][index] = np.std(tmp)\n",
    "            else:\n",
    "                df[column][index] = np.mean(tmp)\n",
    "    return df\n",
    "\n",
    "\n",
    "subreddits = [\"./data/suicidewatch-sample\", \"./data/depression-sample\"]\n",
    "subreddits.sort()\n",
    "\n",
    "\n",
    "ngram_range = (1, 2)\n",
    "fname = \"./data/combinations-10fold.pkl\"\n",
    "combinations = list(itertools.combinations(subreddits, 2))\n",
    "\n",
    "for combination in combinations:\n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname, \"rb\") as f:\n",
    "            rs = pickle.load(f)\n",
    "    else:\n",
    "        rs = {}\n",
    "\n",
    "    if combination[0] in rs.keys():\n",
    "        if combination[1] in rs[combination[0]].keys():\n",
    "            print(combination, \"already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "    print(\"doing\", combination[0], \"-\", combination[1])\n",
    "    df1 = pd.read_pickle(combination[0] + \".pkl\")\n",
    "    df1 = df1.reset_index()\n",
    "    df1[\"text\"] = df1.apply(getTextFromRecord, axis=1)\n",
    "    df1 = df1.reindex(np.random.permutation(df1.index))\n",
    "    df2 = pd.read_pickle(combination[1] + \".pkl\")\n",
    "    df2 = df2.reset_index()\n",
    "    df2[\"text\"] = df2.apply(getTextFromRecord, axis=1)\n",
    "    df2 = df2.reindex(np.random.permutation(df2.index))\n",
    "    # keep only posts, keep only the text column\n",
    "    df1 = df1[df1[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df1 = df1.dropna(subset=[\"text\"])\n",
    "    df2 = df2[df2[\"parent_id\"].astype(str) == \"nan\"]\n",
    "    df2 = df2.dropna(subset=[\"text\"])\n",
    "\n",
    "    results = []\n",
    "    print(\"choosing min from\", len(df1), len(df2))\n",
    "    m = min(len(df1), len(df2))\n",
    "    for i in range(0, 10):\n",
    "        df1_min = df1.reindex(np.random.permutation(df1.index)).head(m)\n",
    "        df2_min = df2.reindex(np.random.permutation(df2.index)).head(m)\n",
    "\n",
    "        data = getData(df1_min, df2_min)\n",
    "        print(\"got\", len(data), \"records...training\")\n",
    "        pipeline = bagOfWords(ngram_range=ngram_range)\n",
    "        pipeline, score = evaluate(pipeline, data=data, getScore=True)\n",
    "        results.append(score)\n",
    "\n",
    "    print(\"RESULTS:\", combination, results)\n",
    "    if combination[0] not in rs.keys():\n",
    "        rs[combination[0]] = {}\n",
    "    rs[combination[0]][combination[1]] = results\n",
    "\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(rs, f)\n",
    "    print(\"finished\", combination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
