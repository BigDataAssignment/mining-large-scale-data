{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/1.5 MB 656.4 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.1/1.5 MB 901.1 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.3/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.3/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.5/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.5/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.0/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.4/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\chris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\chris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/1d/af/4bd17254cdda1d8092460ee5561f013c4ca9c33ecf1aab81b44280327cab/regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\chris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 194.6/268.9 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\chris\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download the words corpus\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the set of English words from NLTK\n",
    "english_words = set(nltk_words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing \n",
    "def clean_text(text):\n",
    "\n",
    "    # Check if text is a string (handles NaN values)\n",
    "    if isinstance(text, str):\n",
    "         text.lower()\n",
    "    else:\n",
    "        str(text).lower()  # Convert non-string values to lowercase strings\n",
    "    \n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_batch(texts):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for doc in nlp.pipe(texts, batch_size=128):\n",
    "        # Initialize a list to store cleaned tokens for each document\n",
    "        cleaned_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Preprocess each token: convert to lowercase, remove special characters\n",
    "            processed_token = re.sub(r'[^a-zA-Z0-9\\s]', '', token.text.strip().lower())\n",
    "            \n",
    "            # Skip stop words and append cleaned tokens to the list\n",
    "            if not token.is_stop and processed_token:\n",
    "                cleaned_tokens.append(processed_token)\n",
    "        \n",
    "        # Join cleaned tokens to form the cleaned text for each document\n",
    "        cleaned_text = ' '.join(cleaned_tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantic relationships\n",
    "def extract_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in [\"nsubj\", \"ROOT\", \"dobj\"]:  # Adjust based on what relationships you want\n",
    "                # Append the relationship tuple (head, dep, child) to the list\n",
    "                relations.append((token.head.text, token.dep_, token.text))\n",
    "    return relations\n",
    "\n",
    "# Create new features based on semantic relationships\n",
    "def create_new_features(row):\n",
    "    title_relations = extract_relations(row['title'])\n",
    "    text_relations = extract_relations(row['selftext'])\n",
    "    \n",
    "    # Combine the relationships from title and text\n",
    "    all_relations = title_relations + text_relations\n",
    "    \n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 title  \\\n",
      "0  Life is so pointless without others   \n",
      "1                           Cold rage?   \n",
      "2                I donâ€™t know who I am   \n",
      "3              HELP! Opinions! Advice!   \n",
      "4                                 help   \n",
      "\n",
      "                                            selftext subreddit  \n",
      "0  Does anyone else think the most important part...       BPD  \n",
      "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...       BPD  \n",
      "2  My [F20] bf [M20] told me today (after I said ...       BPD  \n",
      "3  Okay, Iâ€™m about to open up about many things I...       BPD  \n",
      "4                                          [removed]       BPD  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('mental_disorders_reddit.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['title', 'selftext', 'subreddit']\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# print(df_subset.head())\n",
    "\n",
    "# Preprocess text columns (title and selftext)\n",
    "df_subset['title'] = clean_text_batch(df_subset['title'].fillna('').tolist())\n",
    "df_subset['selftext'] = clean_text_batch(df_subset['selftext'].fillna('').tolist())\n",
    "\n",
    "# Display a preview the preprocessed DataFrame\n",
    "print(df_subset.head())\n",
    "\n",
    "# Apply the function to create a new feature 'semantic_relationships'\n",
    "df_subset['semantic_relationships'] = df_subset.apply(create_new_features, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the new feature\n",
    "print(df_subset.head())\n",
    "\n",
    "#convert df to csv for future analysis\n",
    "df_subset.to_csv('SemanticsRel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subreddit                             semantic_relationships\n",
      "0       BPD  [(pointless, ROOT, pointless), (think, ROOT, t...\n",
      "1       BPD  [(rage, ROOT, rage), (discouraged, nsubj, spec...\n",
      "2       BPD  [(know, ROOT, know), (told, nsubj, m20), (told...\n",
      "3       BPD  [(help, ROOT, help), (help, dobj, opinions), (...\n",
      "4       BPD     [(help, ROOT, help), (removed, ROOT, removed)]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df_svo = pd.read_csv('SemanticsRel.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['subreddit','semantic_relationships']\n",
    "df_svo = df_svo[columns_needed].copy()\n",
    "\n",
    "# Filter out rows with empty relationships\n",
    "df_svo = df_svo[df_svo['semantic_relationships'].apply(len) > 0]\n",
    "\n",
    "# Convert string representations to actual lists of tuples\n",
    "df_svo['semantic_relationships'] = df_svo['semantic_relationships'].apply(ast.literal_eval)\n",
    "\n",
    "print(df_svo.head())\n",
    "# df_svo.to_csv('svo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Subreddit      Word1 Dependency         Word2\n",
      "0                  BPD  pointless       ROOT     pointless\n",
      "1                  BPD      think       ROOT         think\n",
      "2                  BPD      think       dobj  relationship\n",
      "3                  BPD        ask      nsubj     therapist\n",
      "4                  BPD    imagine      nsubj         goals\n",
      "...                ...        ...        ...           ...\n",
      "9979456  mentalillness      think      nsubj           run\n",
      "9979457  mentalillness     having       dobj         think\n",
      "9979458  mentalillness       kill       dobj       uranium\n",
      "9979459  mentalillness  construct       dobj            fu\n",
      "9979460  mentalillness       fuck       ROOT          fuck\n",
      "\n",
      "[9979461 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "\n",
    "for idx, row in df_svo.iterrows():\n",
    "    subreddit = row['subreddit']\n",
    "    relationships = row['semantic_relationships']\n",
    "    \n",
    "    for rel in relationships:\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "            word1, dep, word2 = rel\n",
    "            clean_data.append({'Subreddit': subreddit, 'Word1': word1, 'Dependency': dep, 'Word2': word2})\n",
    "        else:\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Subreddit_x    Word1 Dependency           Word2  Occurrence  \\\n",
      "0             Anxiety  abdomen       ROOT         abdomen           5   \n",
      "1             Anxiety     abit       ROOT            abit           4   \n",
      "2             Anxiety     able       dobj         anxiety          12   \n",
      "3             Anxiety     able       dobj          breath           6   \n",
      "4             Anxiety     able       dobj         breathe          14   \n",
      "...               ...      ...        ...             ...         ...   \n",
      "109713  schizophrenia    worms      nsubj            skin           5   \n",
      "109714  schizophrenia    worse      nsubj  hallucinations           6   \n",
      "109715  schizophrenia    worse      nsubj          voices           5   \n",
      "109716  schizophrenia  writing       dobj   schizophrenia          13   \n",
      "109717  schizophrenia  written       dobj           words           4   \n",
      "\n",
      "        Subreddit_y  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 1  \n",
      "3                 1  \n",
      "4                 1  \n",
      "...             ...  \n",
      "109713            1  \n",
      "109714            1  \n",
      "109715            1  \n",
      "109716            1  \n",
      "109717            1  \n",
      "\n",
      "[109718 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = clean_df.groupby(['Subreddit', 'Word1', 'Dependency', 'Word2']).size().reset_index(name='Occurrence')\n",
    "\n",
    "# Filter out entries where Word1 and Word2 are not in the English dictionary\n",
    "# filtered_relationships = multiple_subreddit_relationships[\n",
    "#     (multiple_subreddit_relationships['Word1'].isin(english_words)) &\n",
    "#     (multiple_subreddit_relationships['Word2'].isin(english_words))\n",
    "# ]\n",
    "\n",
    "# Define a regular expression pattern to match only alphanumeric words\n",
    "pattern = re.compile(r'^[a-zA-Z]+$')\n",
    "\n",
    "# Filter out entries where Word1 and Word2 contain only alphanumeric characters\n",
    "filtered_relationships = grouped[\n",
    "    (grouped['Word1'].str.match(pattern)) &\n",
    "    (grouped['Word2'].str.match(pattern))\n",
    "]\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = filtered_relationships[filtered_relationships['Occurrence'] > 3]\n",
    "\n",
    "\n",
    "# Grouping the DataFrame by 'Word1', 'Dependency', and 'Word2' to count unique occurrences across subreddits\n",
    "relationship_counts = common_relationships.groupby(['Word1', 'Dependency', 'Word2']).agg({'Subreddit': 'nunique'}).reset_index()\n",
    "\n",
    "# Filter relationships occurring in only one subreddit\n",
    "single_subreddit_relationships = relationship_counts[relationship_counts['Subreddit'] == 1]\n",
    "\n",
    "# Merge to keep only the relationships occurring in one subreddit from common_relationships\n",
    "filtered_common_relationships = pd.merge(common_relationships, single_subreddit_relationships, on=['Word1', 'Dependency', 'Word2'], how='inner')\n",
    "\n",
    "print(filtered_common_relationships)\n",
    "filtered_common_relationships.to_csv('SemanticsRelFiltered.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Count occurrences of SVO relationships within each subreddit\n",
    "# subreddit_counts = defaultdict(Counter)\n",
    "# for idx, row in df_svo.iterrows():\n",
    "#     subreddit = row['subreddit']\n",
    "#     for svo in row['semantic_relationships']:\n",
    "#         subreddit_counts[subreddit][svo] += 1\n",
    "\n",
    "# # Filter SVO relationships with count > 3 within each subreddit\n",
    "# frequent_relationships = {}\n",
    "# for subreddit, svo_counter in subreddit_counts.items():\n",
    "#     frequent_relationships[subreddit] = {\n",
    "#         svo: count for svo, count in svo_counter.items() if count > 3\n",
    "#     }\n",
    "\n",
    "# # Display frequent relationships per subreddit\n",
    "# for subreddit, relationships in frequent_relationships.items():\n",
    "#     print(f\"Subreddit: {subreddit}\")\n",
    "#     for svo, count in relationships.items():\n",
    "#         print(f\"SVO: {svo}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries have word1 and word2 which are not actual words but numbers or mispelled words. Even though there was a chance that these were to be filtered due to occurrence should be more than 3, some weren't. At this point i was going to use the NLTK library to check for english words, but upon implementing, some important words like 'zyprexa' was eliminated since it's not included in the corpus. However this is an important entry and therefore should remain. a more crude filter, removing numbers only entries was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "'zyprexia' in english_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
