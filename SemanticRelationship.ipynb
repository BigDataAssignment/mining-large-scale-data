{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Relationship extraction\n",
    "\n",
    "By christian Spiteri Gauci\n",
    "\n",
    "## Literature and References\n",
    "\n",
    "## Method\n",
    "\n",
    "* Cleaning of data - Remove special characters, punctuation, and extra whitespace\n",
    "* Tokenise using spacy\n",
    "* Lemmatize tokens and remove stopwords\n",
    "* extract relationships - using \"nsubj\", \"ROOT\", \"dobj\"\n",
    "* Clean the extracted relationships - remove any entries that inlcude only special characters or numbers\n",
    "* Remove entries that occur only once - and keep those that occur 3times or more in each subreddit \n",
    "* Remove entries that occur in more than one subreddit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "from lib.sanitze_util import clean_text_batch\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download the words corpus\n",
    "nltk.download(\"words\")\n",
    "\n",
    "# Get the set of English words from NLTK\n",
    "english_words = set(nltk_words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def clean_text(text):\n",
    "    # Check if text is a string (handles NaN values)\n",
    "    if isinstance(text, str):\n",
    "        text.lower()\n",
    "    else:\n",
    "        str(text).lower()  # Convert non-string values to lowercase strings\n",
    "\n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize tokens and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_relations(text):\n",
    "    MH_NER = [\n",
    "        \"MENTAL_HEALTH_TERM\",\n",
    "        \"SYMPTOM\",\n",
    "        \"TREATMENT\",\n",
    "    ]\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in MH_NER:\n",
    "            relations.append((ent.text, ent.label_))\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in [\"nsubj\", \"ROOT\", \"dobj\"]:\n",
    "                relations.append((token.head.text, token.dep_, token.text))\n",
    "\n",
    "    return relations\n",
    "\n",
    "def create_new_features(row):\n",
    "    title_relations = extract_relations(row[\"title\"])\n",
    "    text_relations = extract_relations(row[\"selftext\"])\n",
    "    all_relations = title_relations + text_relations\n",
    "    return all_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"./data/mental_disorders_reddit.csv\")\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = [\"title\", \"selftext\", \"subreddit\"]\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# print(df_subset.head())\n",
    "\n",
    "# Preprocess text columns (title and selftext)\n",
    "df_subset[\"title\"] = clean_text_batch(df_subset[\"title\"].fillna(\"\").tolist())\n",
    "df_subset[\"selftext\"] = clean_text_batch(df_subset[\"selftext\"].fillna(\"\").tolist())\n",
    "\n",
    "# Display a preview the preprocessed DataFrame\n",
    "print(df_subset.head())\n",
    "\n",
    "# Apply the function to create a new feature 'semantic_relationships'\n",
    "df_subset[\"semantic_relationships\"] = df_subset.apply(create_new_features, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the new feature\n",
    "print(df_subset.head())\n",
    "\n",
    "# convert df to csv for future analysis\n",
    "df_subset.to_csv(\"SemanticsRel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df_svo = pd.read_csv(\"./data/SemanticsRel.csv\")\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = [\"subreddit\", \"semantic_relationships\"]\n",
    "df_svo = df_svo[columns_needed].copy()\n",
    "\n",
    "# Filter out rows with empty relationships\n",
    "df_svo = df_svo[df_svo[\"semantic_relationships\"].apply(len) > 0]\n",
    "\n",
    "# Convert string representations to actual lists of tuples\n",
    "df_svo[\"semantic_relationships\"] = df_svo[\"semantic_relationships\"].apply(\n",
    "    ast.literal_eval\n",
    ")\n",
    "\n",
    "print(df_svo.head())\n",
    "# df_svo.to_csv('svo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df_svo.iterrows()):\n",
    "\n",
    "    subreddit = row[\"subreddit\"]\n",
    "\n",
    "    relationships = row[\"semantic_relationships\"]\n",
    "\n",
    "    for rel in relationships:\n",
    "\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "\n",
    "            word1, dep, word2 = rel\n",
    "\n",
    "            clean_data.append(\n",
    "                {\n",
    "                    \"Subreddit\": subreddit,\n",
    "                    \"Word1\": word1,\n",
    "                    \"Dependency\": dep,\n",
    "                    \"Word2\": word2,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = (\n",
    "    clean_df.groupby([\"Subreddit\", \"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Occurrence\")\n",
    ")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 are not in the English dictionary\n",
    "# filtered_relationships = multiple_subreddit_relationships[\n",
    "#     (multiple_subreddit_relationships['Word1'].isin(english_words)) &\n",
    "#     (multiple_subreddit_relationships['Word2'].isin(english_words))\n",
    "# ]\n",
    "\n",
    "# Define a regular expression pattern to match only alphanumeric words\n",
    "pattern = re.compile(r\"^[a-zA-Z]+$\")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 contain only alphanumeric characters\n",
    "filtered_relationships = grouped[\n",
    "    (grouped[\"Word1\"].str.match(pattern)) & (grouped[\"Word2\"].str.match(pattern))\n",
    "]\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = filtered_relationships[filtered_relationships[\"Occurrence\"] > 3]\n",
    "\n",
    "\n",
    "# Grouping the DataFrame by 'Word1', 'Dependency', and 'Word2' to count unique occurrences across subreddits\n",
    "relationship_counts = (\n",
    "    common_relationships.groupby([\"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .agg({\"Subreddit\": \"nunique\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter relationships occurring in only one subreddit\n",
    "single_subreddit_relationships = relationship_counts[\n",
    "    relationship_counts[\"Subreddit\"] == 1\n",
    "]\n",
    "\n",
    "# Merge to keep only the relationships occurring in one subreddit from common_relationships\n",
    "filtered_common_relationships = pd.merge(\n",
    "    common_relationships,\n",
    "    single_subreddit_relationships,\n",
    "    on=[\"Word1\", \"Dependency\", \"Word2\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(filtered_common_relationships)\n",
    "filtered_common_relationships.to_csv(\"SemanticsRelFiltered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries have word1 and word2 which are not actual words but numbers or mispelled words. Even though there was a chance that these were to be filtered due to occurrence should be more than 3, some weren't. At this point i was going to use the NLTK library to check for english words, but upon implementing, some important words like 'zyprexa' was eliminated since it's not included in the corpus. However this is an important entry and therefore should remain. a more crude filter, removing numbers only entries was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\"zyprexa\" in english_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
