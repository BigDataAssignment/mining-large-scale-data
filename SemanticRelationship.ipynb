{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing \n",
    "def clean_text(text):\n",
    "\n",
    "    # Check if text is a string (handles NaN values)\n",
    "    if isinstance(text, str):\n",
    "         text.lower()\n",
    "    else:\n",
    "        str(text).lower()  # Convert non-string values to lowercase strings\n",
    "    \n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_batch(texts):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for doc in nlp.pipe(texts, batch_size=128):\n",
    "        # Initialize a list to store cleaned tokens for each document\n",
    "        cleaned_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Preprocess each token: convert to lowercase, remove special characters\n",
    "            processed_token = re.sub(r'[^a-zA-Z0-9\\s]', '', token.text.strip().lower())\n",
    "            \n",
    "            # Skip stop words and append cleaned tokens to the list\n",
    "            if not token.is_stop and processed_token:\n",
    "                cleaned_tokens.append(processed_token)\n",
    "        \n",
    "        # Join cleaned tokens to form the cleaned text for each document\n",
    "        cleaned_text = ' '.join(cleaned_tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantic relationships\n",
    "def extract_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in [\"nsubj\", \"ROOT\", \"dobj\"]:  # Adjust based on what relationships you want\n",
    "                # Append the relationship tuple (head, dep, child) to the list\n",
    "                relations.append((token.head.text, token.dep_, token.text))\n",
    "    return relations\n",
    "\n",
    "# Create new features based on semantic relationships\n",
    "def create_new_features(row):\n",
    "    title_relations = extract_relations(row['title'])\n",
    "    text_relations = extract_relations(row['selftext'])\n",
    "    \n",
    "    # Combine the relationships from title and text\n",
    "    all_relations = title_relations + text_relations\n",
    "    \n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 title  \\\n",
      "0  Life is so pointless without others   \n",
      "1                           Cold rage?   \n",
      "2                I donâ€™t know who I am   \n",
      "3              HELP! Opinions! Advice!   \n",
      "4                                 help   \n",
      "\n",
      "                                            selftext subreddit  \n",
      "0  Does anyone else think the most important part...       BPD  \n",
      "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...       BPD  \n",
      "2  My [F20] bf [M20] told me today (after I said ...       BPD  \n",
      "3  Okay, Iâ€™m about to open up about many things I...       BPD  \n",
      "4                                          [removed]       BPD  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('mental_disorders_reddit.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['title', 'selftext', 'subreddit']\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# print(df_subset.head())\n",
    "\n",
    "# Preprocess text columns (title and selftext)\n",
    "df_subset['title'] = clean_text_batch(df_subset['title'].fillna('').tolist())\n",
    "df_subset['selftext'] = clean_text_batch(df_subset['selftext'].fillna('').tolist())\n",
    "\n",
    "# Display a preview the preprocessed DataFrame\n",
    "print(df_subset.head())\n",
    "\n",
    "# Apply the function to create a new feature 'semantic_relationships'\n",
    "df_subset['semantic_relationships'] = df_subset.apply(create_new_features, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the new feature\n",
    "print(df_subset.head())\n",
    "\n",
    "#convert df to csv for future analysis\n",
    "df_subset.to_csv('SemanticsRel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subreddit                             semantic_relationships\n",
      "0       BPD  [(pointless, ROOT, pointless), (think, ROOT, t...\n",
      "1       BPD  [(rage, ROOT, rage), (discouraged, nsubj, spec...\n",
      "2       BPD  [(know, ROOT, know), (told, nsubj, m20), (told...\n",
      "3       BPD  [(help, ROOT, help), (help, dobj, opinions), (...\n",
      "4       BPD     [(help, ROOT, help), (removed, ROOT, removed)]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df_svo = pd.read_csv('SemanticsRel.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['subreddit','semantic_relationships']\n",
    "df_svo = df_svo[columns_needed].copy()\n",
    "\n",
    "# Filter out rows with empty relationships\n",
    "df_svo = df_svo[df_svo['semantic_relationships'].apply(len) > 0]\n",
    "\n",
    "# Convert string representations to actual lists of tuples\n",
    "df_svo['semantic_relationships'] = df_svo['semantic_relationships'].apply(ast.literal_eval)\n",
    "\n",
    "print(df_svo.head())\n",
    "# df_svo.to_csv('svo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Subreddit      Word1 Dependency         Word2\n",
      "0                  BPD  pointless       ROOT     pointless\n",
      "1                  BPD      think       ROOT         think\n",
      "2                  BPD      think       dobj  relationship\n",
      "3                  BPD        ask      nsubj     therapist\n",
      "4                  BPD    imagine      nsubj         goals\n",
      "...                ...        ...        ...           ...\n",
      "9979456  mentalillness      think      nsubj           run\n",
      "9979457  mentalillness     having       dobj         think\n",
      "9979458  mentalillness       kill       dobj       uranium\n",
      "9979459  mentalillness  construct       dobj            fu\n",
      "9979460  mentalillness       fuck       ROOT          fuck\n",
      "\n",
      "[9979461 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "\n",
    "for idx, row in df_svo.iterrows():\n",
    "    subreddit = row['subreddit']\n",
    "    relationships = row['semantic_relationships']\n",
    "    \n",
    "    for rel in relationships:\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "            word1, dep, word2 = rel\n",
    "            clean_data.append({'Subreddit': subreddit, 'Word1': word1, 'Dependency': dep, 'Word2': word2})\n",
    "        else:\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word1 Dependency    Word2  Subreddit_Count\n",
      "1             1       ROOT        1                3\n",
      "8            17       ROOT       17                2\n",
      "10           18       ROOT       18                3\n",
      "11          18f       ROOT      18f                2\n",
      "13          19f       ROOT      19f                2\n",
      "...         ...        ...      ...              ...\n",
      "187055   zombie       ROOT   zombie                3\n",
      "187056     zone       ROOT     zone                3\n",
      "187058   zoning       ROOT   zoning                2\n",
      "187059     zoom       ROOT     zoom                2\n",
      "187061  zyprexa       ROOT  zyprexa                6\n",
      "\n",
      "[76895 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = clean_df.groupby(['Subreddit', 'Word1', 'Dependency', 'Word2']).size().reset_index(name='Occurrence')\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = grouped[grouped['Occurrence'] > 3]\n",
    "\n",
    "# Grouping by 'Word1', 'Dependency', and 'Word2' to count unique occurrences\n",
    "relationship_counts = common_relationships.groupby(['Word1', 'Dependency', 'Word2']).size().reset_index(name='Subreddit_Count')\n",
    "\n",
    "# Filtering relationships occurring in more than one subreddit\n",
    "multiple_subreddit_relationships = relationship_counts[relationship_counts['Subreddit_Count'] > 1]\n",
    "\n",
    "print(multiple_subreddit_relationships)\n",
    "\n",
    "\n",
    "# # Count occurrences of SVO relationships within each subreddit\n",
    "# subreddit_counts = defaultdict(Counter)\n",
    "# for idx, row in df_svo.iterrows():\n",
    "#     subreddit = row['subreddit']\n",
    "#     for svo in row['semantic_relationships']:\n",
    "#         subreddit_counts[subreddit][svo] += 1\n",
    "\n",
    "# # Filter SVO relationships with count > 3 within each subreddit\n",
    "# frequent_relationships = {}\n",
    "# for subreddit, svo_counter in subreddit_counts.items():\n",
    "#     frequent_relationships[subreddit] = {\n",
    "#         svo: count for svo, count in svo_counter.items() if count > 3\n",
    "#     }\n",
    "\n",
    "# # Display frequent relationships per subreddit\n",
    "# for subreddit, relationships in frequent_relationships.items():\n",
    "#     print(f\"Subreddit: {subreddit}\")\n",
    "#     for svo, count in relationships.items():\n",
    "#         print(f\"SVO: {svo}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
