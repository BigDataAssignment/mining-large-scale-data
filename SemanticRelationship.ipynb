{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing \n",
    "def clean_text(text):\n",
    "\n",
    "    # Check if text is a string (handles NaN values)\n",
    "    if isinstance(text, str):\n",
    "         text.lower()\n",
    "    else:\n",
    "        str(text).lower()  # Convert non-string values to lowercase strings\n",
    "    \n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_batch(texts):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for doc in nlp.pipe(texts, batch_size=128):\n",
    "        # Initialize a list to store cleaned tokens for each document\n",
    "        cleaned_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Preprocess each token: convert to lowercase, remove special characters\n",
    "            processed_token = re.sub(r'[^a-zA-Z0-9\\s]', '', token.text.strip().lower())\n",
    "            \n",
    "            # Skip stop words and append cleaned tokens to the list\n",
    "            if not token.is_stop and processed_token:\n",
    "                cleaned_tokens.append(processed_token)\n",
    "        \n",
    "        # Join cleaned tokens to form the cleaned text for each document\n",
    "        cleaned_text = ' '.join(cleaned_tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantic relationships\n",
    "def extract_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in [\"nsubj\", \"ROOT\", \"dobj\"]:  # Adjust based on what relationships you want\n",
    "                # Append the relationship tuple (head, dep, child) to the list\n",
    "                relations.append((token.head.text, token.dep_, token.text))\n",
    "    return relations\n",
    "\n",
    "# Create new features based on semantic relationships\n",
    "def create_new_features(row):\n",
    "    title_relations = extract_relations(row['title'])\n",
    "    text_relations = extract_relations(row['selftext'])\n",
    "    \n",
    "    # Combine the relationships from title and text\n",
    "    all_relations = title_relations + text_relations\n",
    "    \n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 title  \\\n",
      "0  Life is so pointless without others   \n",
      "1                           Cold rage?   \n",
      "2                I donâ€™t know who I am   \n",
      "3              HELP! Opinions! Advice!   \n",
      "4                                 help   \n",
      "\n",
      "                                            selftext subreddit  \n",
      "0  Does anyone else think the most important part...       BPD  \n",
      "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...       BPD  \n",
      "2  My [F20] bf [M20] told me today (after I said ...       BPD  \n",
      "3  Okay, Iâ€™m about to open up about many things I...       BPD  \n",
      "4                                          [removed]       BPD  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('mental_disorders_reddit.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['title', 'selftext', 'subreddit']\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# print(df_subset.head())\n",
    "\n",
    "# Preprocess text columns (title and selftext)\n",
    "df_subset['title'] = clean_text_batch(df_subset['title'].fillna('').tolist())\n",
    "df_subset['selftext'] = clean_text_batch(df_subset['selftext'].fillna('').tolist())\n",
    "\n",
    "# Display a preview the preprocessed DataFrame\n",
    "print(df_subset.head())\n",
    "\n",
    "# Apply the function to create a new feature 'semantic_relationships'\n",
    "df_subset['semantic_relationships'] = df_subset.apply(create_new_features, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the new feature\n",
    "print(df_subset.head())\n",
    "\n",
    "#convert df to csv for future analysis\n",
    "df_subset.to_csv('SemanticsRel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            subreddit                             semantic_relationships\n",
      "0                 BPD  [('is', 'nsubj', 'life'), ('is', 'ROOT', 'is')...\n",
      "1                 BPD  [('rage', 'ROOT', 'rage'), (\"'m\", 'nsubj', 'fr...\n",
      "2                 BPD  [('know', 'nsubj', 'i'), ('know', 'ROOT', 'kno...\n",
      "3                 BPD  [('help', 'ROOT', 'help'), ('opinions', 'ROOT'...\n",
      "4                 BPD  [('help', 'ROOT', 'help'), ('removed', 'ROOT',...\n",
      "...               ...                                                ...\n",
      "701782  mentalillness  [('go', 'nsubj', 'you'), ('go', 'ROOT', 'go'),...\n",
      "701783  mentalillness  [('am', 'nsubj', 'i'), ('am', 'ROOT', 'am'), (...\n",
      "701784  mentalillness  [('look', 'nsubj', 'i'), ('look', 'ROOT', 'loo...\n",
      "701785  mentalillness  [('is', 'nsubj', '|'), ('is', 'ROOT', 'is'), (...\n",
      "701786  mentalillness  [('motherfucker', 'ROOT', 'motherfucker'), ('h...\n",
      "\n",
      "[701787 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df_svo = pd.read_csv('SemanticsRel.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['subreddit','semantic_relationships']\n",
    "df_svo = df_svo[columns_needed].copy()\n",
    "\n",
    "# Filter out rows with empty relationships\n",
    "df_svo = df_svo[df_svo['semantic_relationships'].apply(len) > 0]\n",
    "\n",
    "print(df_svo.head())\n",
    "# df_svo.to_csv('svo.csv', index=False)\n",
    "\n",
    "#still need to do some filtering\n",
    "\n",
    "# # Count occurrences of SVO relationships within each subreddit\n",
    "# subreddit_counts = defaultdict(Counter)\n",
    "# for idx, row in df_svo.iterrows():\n",
    "#     subreddit = row['subreddit']\n",
    "#     for svo in row['semantic_relationships']:\n",
    "#         subreddit_counts[subreddit][svo] += 1\n",
    "\n",
    "# # Filter SVO relationships with count > 3 within each subreddit\n",
    "# frequent_relationships = {}\n",
    "# for subreddit, svo_counter in subreddit_counts.items():\n",
    "#     frequent_relationships[subreddit] = {\n",
    "#         svo: count for svo, count in svo_counter.items() if count > 3\n",
    "#     }\n",
    "\n",
    "# # Display frequent relationships per subreddit\n",
    "# for subreddit, relationships in frequent_relationships.items():\n",
    "#     print(f\"Subreddit: {subreddit}\")\n",
    "#     for svo, count in relationships.items():\n",
    "#         print(f\"SVO: {svo}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
