{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Adam:\n",
    "\n",
    "Let's collect all preprocessing steps in lib/sanitize_util.py\n",
    "\n",
    "to use my functions:\n",
    "\n",
    "```\n",
    "from lib.sanitze_util import clean_text_batch\n",
    "\n",
    "cleaned_texts = clean_text_batch(df[\"body\"], multi_proc=True)\n",
    "```\n",
    "\n",
    "read the functions documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing \n",
    "def clean_text(text):\n",
    "\n",
    "    # Check if text is a string (handles NaN values)\n",
    "    if isinstance(text, str):\n",
    "         text.lower()\n",
    "    else:\n",
    "        str(text).lower()  # Convert non-string values to lowercase strings\n",
    "    \n",
    "    # Remove special characters, punctuation, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize tokens and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_batch(texts):\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for doc in nlp.pipe(texts, batch_size=128):\n",
    "        # Initialize a list to store cleaned tokens for each document\n",
    "        cleaned_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Preprocess each token: convert to lowercase, remove special characters\n",
    "            processed_token = re.sub(r'[^a-zA-Z0-9\\s]', '', token.text.strip().lower())\n",
    "            \n",
    "            # Skip stop words and append cleaned tokens to the list\n",
    "            if not token.is_stop and processed_token:\n",
    "                cleaned_tokens.append(processed_token)\n",
    "        \n",
    "        # Join cleaned tokens to form the cleaned text for each document\n",
    "        cleaned_text = ' '.join(cleaned_tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract semantic relationships\n",
    "def extract_relations(text):\n",
    "    doc = nlp(text)\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in [\"nsubj\", \"ROOT\", \"dobj\"]:  # Adjust based on what relationships you want\n",
    "                # Append the relationship tuple (head, dep, child) to the list\n",
    "                relations.append((token.head.text, token.dep_, token.text))\n",
    "    return relations\n",
    "\n",
    "# Create new features based on semantic relationships\n",
    "def create_new_features(row):\n",
    "    title_relations = extract_relations(row['title'])\n",
    "    text_relations = extract_relations(row['selftext'])\n",
    "    \n",
    "    # Combine the relationships from title and text\n",
    "    all_relations = title_relations + text_relations\n",
    "    \n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('mental_disorders_reddit.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['title', 'selftext', 'subreddit']\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# print(df_subset.head())\n",
    "\n",
    "# Preprocess text columns (title and selftext)\n",
    "df_subset['title'] = clean_text_batch(df_subset['title'].fillna('').tolist())\n",
    "df_subset['selftext'] = clean_text_batch(df_subset['selftext'].fillna('').tolist())\n",
    "\n",
    "# Display a preview the preprocessed DataFrame\n",
    "print(df_subset.head())\n",
    "\n",
    "# Apply the function to create a new feature 'semantic_relationships'\n",
    "df_subset['semantic_relationships'] = df_subset.apply(create_new_features, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the new feature\n",
    "print(df_subset.head())\n",
    "\n",
    "#convert df to csv for future analysis\n",
    "df_subset.to_csv('SemanticsRel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df_svo = pd.read_csv('SemanticsRel.csv')\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = ['subreddit','semantic_relationships']\n",
    "df_svo = df_svo[columns_needed].copy()\n",
    "\n",
    "# Filter out rows with empty relationships\n",
    "df_svo = df_svo[df_svo['semantic_relationships'].apply(len) > 0]\n",
    "\n",
    "# Convert string representations to actual lists of tuples\n",
    "df_svo['semantic_relationships'] = df_svo['semantic_relationships'].apply(ast.literal_eval)\n",
    "\n",
    "print(df_svo.head())\n",
    "# df_svo.to_csv('svo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "\n",
    "for idx, row in df_svo.iterrows():\n",
    "    subreddit = row['subreddit']\n",
    "    relationships = row['semantic_relationships']\n",
    "    \n",
    "    for rel in relationships:\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "            word1, dep, word2 = rel\n",
    "            clean_data.append({'Subreddit': subreddit, 'Word1': word1, 'Dependency': dep, 'Word2': word2})\n",
    "        else:\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = clean_df.groupby(['Subreddit', 'Word1', 'Dependency', 'Word2']).size().reset_index(name='Occurrence')\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = grouped[grouped['Occurrence'] > 3]\n",
    "\n",
    "# Grouping by 'Word1', 'Dependency', and 'Word2' to count unique occurrences\n",
    "relationship_counts = common_relationships.groupby(['Word1', 'Dependency', 'Word2']).size().reset_index(name='Subreddit_Count')\n",
    "\n",
    "# Filtering relationships occurring in more than one subreddit\n",
    "multiple_subreddit_relationships = relationship_counts[relationship_counts['Subreddit_Count'] > 1]\n",
    "\n",
    "print(multiple_subreddit_relationships)\n",
    "\n",
    "\n",
    "# # Count occurrences of SVO relationships within each subreddit\n",
    "# subreddit_counts = defaultdict(Counter)\n",
    "# for idx, row in df_svo.iterrows():\n",
    "#     subreddit = row['subreddit']\n",
    "#     for svo in row['semantic_relationships']:\n",
    "#         subreddit_counts[subreddit][svo] += 1\n",
    "\n",
    "# # Filter SVO relationships with count > 3 within each subreddit\n",
    "# frequent_relationships = {}\n",
    "# for subreddit, svo_counter in subreddit_counts.items():\n",
    "#     frequent_relationships[subreddit] = {\n",
    "#         svo: count for svo, count in svo_counter.items() if count > 3\n",
    "#     }\n",
    "\n",
    "# # Display frequent relationships per subreddit\n",
    "# for subreddit, relationships in frequent_relationships.items():\n",
    "#     print(f\"Subreddit: {subreddit}\")\n",
    "#     for svo, count in relationships.items():\n",
    "#         print(f\"SVO: {svo}, Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
