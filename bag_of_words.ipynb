{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "Created by Owen Fava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download (\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_dataset(mental_health_disorder: str, dataset_path: str, features: list[str]):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    cleaned_dataset = pd.DataFrame()\n",
    "    for feature in features:\n",
    "        cleaned_dataset[feature] = dataset[feature]\n",
    "    \n",
    "    return cleaned_dataset.dropna()\n",
    "\n",
    "cleaned_dataset = get_and_clean_dataset(\"depression\", \"data/depression-sample.csv\", [\"title\", \"selftext\"])\n",
    "cleaned_dataset.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(data):\n",
    "    tokenisation = [word for sentence in data for word in word_tokenize(sentence)]\n",
    "    return tokenisation\n",
    "\n",
    "title_data =  (tokenize_text(cleaned_dataset[\"title\"]))\n",
    "selftext_data =  (tokenize_text(cleaned_dataset[\"selftext\"]))\n",
    "dataset = title_data + selftext_data\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(data, toLower: bool):\n",
    "    result = []\n",
    "\n",
    "    if toLower:\n",
    "        result = [text.lower() for text in data]\n",
    "    else:\n",
    "        result = [text.upper() for text in data]\n",
    "\n",
    "    return result\n",
    "\n",
    "dataset = case_folding(dataset, True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_non_alphabetical_words(data):\n",
    "    result = [text for text in data if text.isalpha()]\n",
    "    return result\n",
    "\n",
    "print(len(dataset))\n",
    "dataset = discard_non_alphabetical_words(dataset)\n",
    "print(dataset)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    result = [text for text in data if (not text in stopwords.words(\"english\"))]\n",
    "    return result\n",
    "\n",
    "print(len(dataset))\n",
    "dataset = remove_stop_words(dataset)\n",
    "print(dataset)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(text) for text in data]\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "def get_lemmas_stats(lemmas):\n",
    "    stats = []\n",
    "\n",
    "    for term in lemmas:\n",
    "        term_frequency = list(filter(lambda t: t[\"term\"] == term, stats))\n",
    "\n",
    "        if(len(term_frequency) > 0):\n",
    "            term_frequency = term_frequency[0]\n",
    "        else:\n",
    "            term_frequency = { \"term\": term, \"frequency\": 0 }\n",
    "            stats.append(term_frequency)\n",
    "\n",
    "        term_frequency[\"frequency\"] += 1\n",
    "\n",
    "    return sorted(stats, key=lambda f: f[\"frequency\"], reverse=True)\n",
    "\n",
    "lemmas = lemmatize_text(dataset)\n",
    "print(len(get_lemmas_stats(lemmas)));\n",
    "print(json.dumps(get_lemmas_stats(lemmas), indent=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bag_of_words(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    x = vectorizer.fit_transform(data)\n",
    "\n",
    "    word_frequencies = list(zip(vectorizer.get_feature_names_out(), vectorizer.transform(data).sum(axis=0).tolist()[0]))\n",
    "    word_frequencies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return x.toarray(), vectorizer.get_feature_names_out(), word_frequencies\n",
    "\n",
    "bag_of_words_matrix, feature_names, word_frequencies = generate_bag_of_words(dataset)\n",
    "print(\"Bag of Words Matrix: \\n\", bag_of_words_matrix)\n",
    "print(\"\\nFeature Names: \\n\", feature_names)\n",
    "print(\"\\nWord Frequencies: \\n\", word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bag_of_words_visual(data):\n",
    "    words = []\n",
    "    for text in data:\n",
    "        words.append(text)\n",
    "\n",
    "    visual = ' '.join(words)\n",
    "\n",
    "    wordcloud = WordCloud(width=1000, height=600, background_color=\"white\").generate(visual)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "generate_bag_of_words_visual(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_tagging(data):\n",
    "    tokenized_text = tokenize_text(data)\n",
    "    pos_tags = nltk.pos_tag(tokenized_text)\n",
    "    \n",
    "    return pos_tags\n",
    "\n",
    "tags = generate_pos_tagging(cleaned_dataset[\"title\"] + cleaned_dataset[\"selftext\"])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(tags):\n",
    "    regex = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "    parser = nltk.RegexpParser(regex)\n",
    "    noun_phrase_tree = parser.parse(tags)\n",
    "\n",
    "    phrases = []\n",
    "\n",
    "    for node in noun_phrase_tree.subtrees ():\n",
    "        if (node.label() == \"NP\"):\n",
    "            phrases.append (\" \".join(w for w,t in node.leaves()))\n",
    "\n",
    "    return phrases\n",
    "\n",
    "noun_phrases = extract_noun_phrases(tags)\n",
    "print(noun_phrases)\n",
    "\n",
    "def extract_key_phrases(tags):\n",
    "    regex = \"\"\" NP: {<DT>?<JJ>*<NN.*>+} \n",
    "                KP: {<JJ.*>*<NP>+<IN>*<VBG>*<JJ.*|NP>*} \n",
    "            \"\"\"\n",
    "\n",
    "    parser = nltk.RegexpParser(regex)\n",
    "    key_phrase_tree = parser.parse(tags)\n",
    "\n",
    "    phrases = []\n",
    "\n",
    "    for node in key_phrase_tree.subtrees ():\n",
    "        if (node.label() == \"KP\"):\n",
    "            phrases.append (\" \".join(w for w,t in node.leaves()));\n",
    "\n",
    "    return phrases\n",
    "\n",
    "key_phrases = extract_key_phrases(tags)\n",
    "print(key_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, ngram_value):\n",
    "    ngram = list(ngrams(text, ngram_value))\n",
    "    return ngram\n",
    "\n",
    "print(\"Bigrams: \\n\")\n",
    "print(generate_ngrams(dataset, 2))\n",
    "\n",
    "print(\"Trigrams: \\n\")\n",
    "print(generate_ngrams(dataset, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
