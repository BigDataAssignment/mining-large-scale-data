{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Created by Owen Fava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamd\\AppData\\Local\\Temp\\ipykernel_33996\\1646028301.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adamd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subreddit     Word1 Dependency     Word2  \\\n",
      "0   Anxiety   knowing       dobj       day   \n",
      "1       BPD   boosted       dobj  finances   \n",
      "2   Anxiety    taking       dobj     carbs   \n",
      "3   Anxiety  universe      nsubj     death   \n",
      "4       BPD     heard      nsubj      help   \n",
      "\n",
      "                                            MHlabels  \\\n",
      "0  {'SYMPTOMS': ['panic attack', 'anxiety'], 'SLE...   \n",
      "1  {'ANXIETY DISORDERS': ['anxiety'], 'DEPRESSIVE...   \n",
      "2  {'SYMPTOMS': ['anxiety'], 'ANXIETY DISORDERS':...   \n",
      "3                                                 {}   \n",
      "4                            {'SYMPTOMS': ['anger']}   \n",
      "\n",
      "                                     title  \\\n",
      "0       worried sleeping holiday away home   \n",
      "1                               lost ether   \n",
      "2     thought diet crucial dealing anxiety   \n",
      "3  stop thinking existentialism death time   \n",
      "4                         grief motherload   \n",
      "\n",
      "                                            selfText  \n",
      "0  15 typing night holiday 'staycation' hours awa...  \n",
      "1  hey use reddit kind running usual means busy t...  \n",
      "2  2020 everyday anxiety crippling anxiety stop g...  \n",
      "3  trigger warning think trigger warning upset pe...  \n",
      "4  going worst heart wrenching loss life bpd know...  \n",
      "659\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./data/GraphData_test.csv\")\n",
    "print(dataset.head(5))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   combined_title_selftext\n",
      "0       worried sleeping holiday away home\n",
      "1                               lost ether\n",
      "2     thought diet crucial dealing anxiety\n",
      "3  stop thinking existentialism death time\n",
      "4                         grief motherload\n",
      "1318\n"
     ]
    }
   ],
   "source": [
    "def combine_data_columns(data: str, columns: list[str], new_column_name: str):\n",
    "    combined_data = pd.concat([data[column] for column in columns], ignore_index=True)\n",
    "    combined_cleaned_data = pd.DataFrame({new_column_name: combined_data})\n",
    "\n",
    "    return combined_cleaned_data\n",
    "\n",
    "data = combine_data_columns(dataset, [\"title\", \"selfText\"], \"combined_title_selftext\")\n",
    "print(data.head(5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   combined_title_selftext\n",
      "0       worried sleeping holiday away home\n",
      "1                               lost ether\n",
      "2     thought diet crucial dealing anxiety\n",
      "3  stop thinking existentialism death time\n",
      "4                         grief motherload\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "def keep_alphabetical_words(data):\n",
    "    if pd.isna(data):\n",
    "        return ''\n",
    "\n",
    "    words = data.split()\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "data[\"combined_title_selftext\"] = data[\"combined_title_selftext\"].apply(keep_alphabetical_words)\n",
    "\n",
    "# Remove empty rows\n",
    "data = data[data[\"combined_title_selftext\"].str.len() > 0]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(data.head(5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 n-grams: [('worried', 'sleeping'), ('sleeping', 'holiday'), ('holiday', 'away'), ('away', 'home'), ('lost', 'ether'), ('thought', 'diet'), ('diet', 'crucial'), ('crucial', 'dealing'), ('dealing', 'anxiety'), ('stop', 'thinking')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(sentences, n):\n",
    "    ngram_list = []\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            n_grams = list(ngrams(tokens, n))\n",
    "            ngram_list.extend(n_grams)\n",
    "    return ngram_list\n",
    "\n",
    "n_grams_data = generate_ngrams(sentences=data[\"combined_title_selftext\"], n=2)\n",
    "print(\"First 10 n-grams:\", n_grams_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference of Word2Vec: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             combined_title_selftext\n",
      "0                 worried sleeping holiday away home\n",
      "1                                         lost ether\n",
      "2               thought diet crucial dealing anxiety\n",
      "3            stop thinking existentialism death time\n",
      "4                                   grief motherload\n",
      "5  friend coworker quitting ecstatic terrible person\n",
      "6                       feel frustrated idealization\n",
      "7            family ruined life know fix point think\n",
      "8                            stop frowning gibberish\n",
      "9  thoughts accepting reality living modern socie...\n",
      "[['worried', 'sleeping', 'holiday', 'away', 'home'], ['lost', 'ether'], ['thought', 'diet', 'crucial', 'dealing', 'anxiety'], ['stop', 'thinking', 'existentialism', 'death', 'time'], ['grief', 'motherload'], ['friend', 'coworker', 'quitting', 'ecstatic', 'terrible', 'person'], ['feel', 'frustrated', 'idealization'], ['family', 'ruined', 'life', 'know', 'fix', 'point', 'think'], ['stop', 'frowning', 'gibberish'], ['thoughts', 'accepting', 'reality', 'living', 'modern', 'society', 'depression']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamd\\.conda\\envs\\gnn\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    model_data.append(utils.simple_preprocess(row.iloc[0]))\n",
    "\n",
    "print(data.head(10))\n",
    "print(model_data[:10])\n",
    "\n",
    "model = Word2Vec(sentences=model_data, window=2, min_count=3)\n",
    "\n",
    "model.save(\"./data/word2vec_embeddings.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary: 4108\n",
      "\n",
      "Vector for anxiety:  [ 0.91743124 -0.15346275  0.5612611   0.12110069 -0.36633033 -0.027039\n",
      "  0.766288   -0.01436418  0.5586183  -0.30250457 -0.5072509  -0.10233949\n",
      " -0.05358497 -0.19796601  0.11952449 -0.05286039 -0.81796783  0.06031595\n",
      "  0.993565    0.32394123  0.01161638 -0.43665498 -0.19606447 -0.5011399\n",
      " -0.28716308 -0.33631784  0.30174196 -0.48437116  0.00257769 -0.13986856\n",
      "  0.13831638  0.5599948   0.02161547 -0.65895987  0.29117948 -0.09961222\n",
      "  0.52416235  0.47608513  0.13760327  0.10166731  0.04950099 -0.2869896\n",
      "  0.04365897 -0.41008204 -0.55863047  0.34194645  0.09776545 -0.40645927\n",
      "  0.37375703 -0.41520548 -0.2868897   0.6229352   0.18483339 -0.68089205\n",
      "  0.180556   -0.37897637 -0.4665601   0.16920866 -0.41236973 -0.32825047\n",
      " -0.37534845 -0.26796955  0.19134629  0.41265154  0.06596199 -0.13089637\n",
      " -0.33334473 -0.06814519  0.25660405 -0.17864305  0.2386284  -0.13914566\n",
      " -0.09507644 -0.37319258  0.28276768 -0.24656092 -0.10530248 -0.2839538\n",
      " -0.49123862  0.47421196 -0.42179894 -0.5490197  -0.25783736  0.79524255\n",
      "  0.38549727  0.25175908  0.01573944 -0.05463734 -0.7564657  -0.15072997\n",
      "  0.00669319  0.05787462  0.02837584  0.13026181  0.40910462 -0.3091931\n",
      "  0.05454084  0.606179   -0.5195586   0.02442028]\n",
      "Similar words:  [('started', 0.9998691082000732), ('sure', 0.9998670220375061), ('month', 0.999859094619751), ('world', 0.9998577833175659), ('problem', 0.9998537302017212), ('symptoms', 0.9998507499694824), ('night', 0.9998503923416138), ('head', 0.9998471736907959), ('stopped', 0.9998459219932556), ('talk', 0.9998428821563721)]\n"
     ]
    }
   ],
   "source": [
    "# Accessing the vocabulary\n",
    "vocabulary = model.wv.vocab\n",
    "print(f\"Words in Vocabulary: {len(vocabulary)}\")\n",
    "\n",
    "# Print each word of within the vocabulary\n",
    "# print(\"\\nVocabulary:\")\n",
    "# for word in vocabulary:\n",
    "#     print(word)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_to_find = \"anxiety\"\n",
    "word_vector = model.wv[word_to_find]\n",
    "print(f\"\\nVector for {word_to_find}: \", word_vector)\n",
    "\n",
    "similar_words = model.wv.most_similar(word_to_find)\n",
    "print(\"Similar words: \", similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
