{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Created by Owen Fava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/GraphData_sampled_new.csv\")\n",
    "print(dataset.head(5))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_columns(data: str, columns: list[str], new_column_name: str):\n",
    "    combined_data = pd.concat([data[column] for column in columns], ignore_index=True)\n",
    "    combined_cleaned_data = pd.DataFrame({new_column_name: combined_data})\n",
    "\n",
    "    return combined_cleaned_data\n",
    "\n",
    "data = combine_data_columns(dataset, [\"title\", \"selfText\"], \"combined_title_selftext\")\n",
    "print(data.head(5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_alphabetical_words(data):\n",
    "    if pd.isna(data):\n",
    "        return ''\n",
    "    \n",
    "    words = data.split()\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "data[\"combined_title_selftext\"] = data[\"combined_title_selftext\"].apply(keep_alphabetical_words)\n",
    "\n",
    "# Remove empty rows\n",
    "data = data[data[\"combined_title_selftext\"].str.len() > 0]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print(data.head(5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(sentences, n):\n",
    "    ngram_list = []\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            n_grams = list(ngrams(tokens, n))\n",
    "            ngram_list.extend(n_grams)\n",
    "    return ngram_list\n",
    "\n",
    "n_grams_data = generate_ngrams(sentences=data[\"combined_title_selftext\"], n=2)\n",
    "print(\"First 10 n-grams:\", n_grams_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference of Word2Vec: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=n_grams_data, vector_size=100, window=2, min_count=3)\n",
    "\n",
    "model.save(\"word2vec_model_with_ngrams.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the vocabulary\n",
    "vocabulary = model.wv.key_to_index\n",
    "print(f\"Words in Vocabulary: {len(vocabulary)}\")\n",
    "\n",
    "# Print each word of within the vocabulary\n",
    "# print(\"\\nVocabulary:\")\n",
    "# for word in vocabulary:\n",
    "#     print(word)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "text_to_find = \"anxiety\"\n",
    "word_vector = model.wv[text_to_find]\n",
    "print(f\"\\nVector for {text_to_find}: \", word_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
