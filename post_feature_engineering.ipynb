{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Relationship Extraction and Feature Engineering\n",
    "\n",
    "By christian Spiteri Gauci and Adam Darmanin\n",
    "\n",
    "## Literature and References\n",
    "\n",
    "## Method\n",
    "\n",
    "* Cleaning of data - Remove special characters, punctuation, and extra whitespace\n",
    "* Tokenise using spacy\n",
    "* Lemmatize tokens and remove stopwords\n",
    "* extract relationships - using \"nsubj\", \"ROOT\", \"dobj\"\n",
    "* Clean the extracted relationships - remove any entries that inlcude only special characters or numbers\n",
    "* Remove entries that occur only once - and keep those that occur 3times or more in each subreddit \n",
    "* Remove entries that occur in more than one subreddit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "\n",
    "from lib.sanitze_util import clean_text_batch\n",
    "\n",
    "tqdm.pandas()  # for progressbase in DFs.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "english_words = set(nltk_words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. Clean stopwords\n",
    "2. Lemmatize\n",
    "3. Convert emoticons\n",
    "4. Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_27176\\2031721653.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mask = df_subset.applymap(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
      "Cleaning Pipeline Token:   0%|          | 0/668031 [00:00<?, ?it/s]g:\\My Drive\\1. Educ\\1. Masters\\Year 1\\ICS5111 - Mining Large Scale Data\\Personal Assignment\\gitrepo\\mining-large-scale-data\\lib\\sanitze_util.py:88: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "C:\\Users\\chris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Cleaning Pipeline Token:  92%|█████████▏| 616977/668031 [01:23<00:06, 7701.84it/s]g:\\My Drive\\1. Educ\\1. Masters\\Year 1\\ICS5111 - Mining Large Scale Data\\Personal Assignment\\gitrepo\\mining-large-scale-data\\lib\\sanitze_util.py:88: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Cleaning Pipeline Token: 100%|██████████| 668031/668031 [01:30<00:00, 7368.33it/s]\n",
      "Cleaning Pipeline Token: 100%|██████████| 668031/668031 [21:07<00:00, 526.98it/s] \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/mental_disorders_reddit.csv\")\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = [\"title\", \"selftext\", \"subreddit\"]\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# Drop anything not a full peice of text in this dataset.\n",
    "mask = df_subset.applymap(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "df_subset = df_subset[mask[\"title\"] & mask[\"selftext\"]]\n",
    "df_subset[\"raw_title\"] = df_subset[\"title\"]\n",
    "df_subset[\"raw_selftext\"] = df_subset[\"selftext\"]\n",
    "df_subset[\"title\"] = clean_text_batch(df_subset[\"title\"].fillna(\"\").tolist())\n",
    "df_subset[\"selftext\"] = clean_text_batch(df_subset[\"selftext\"].fillna(\"\").tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "1. Semantic relations between words\n",
    "2. Mental health labels in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 286746/668031 [09:46<11:27, 554.36it/s]"
     ]
    }
   ],
   "source": [
    "# borrowed from the research: https://github.com/zhukovanadezhda/psy-ner/tree/main\n",
    "# Following paper: https://www.researchgate.net/publication/358779855_Deep_Learning-based_Detection_of_Psychiatric_Attributes_from_German_Mental_Health_Records\n",
    "# see: https://spacy.io/usage/processing-pipelines\n",
    "psy_ner = spacy.load(\"./model/psy_ner\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from empath import Empath\n",
    "\n",
    "lexicon = Empath()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "MH_NER = [\n",
    "    \"ANXIETY DISORDERS\",\n",
    "    \"BIPOLAR DISORDERS\",\n",
    "    \"DEPRESSIVE DISORDERS\",\n",
    "    \"DISRUPTIVE IMPULSE-CONTROL, AND CONDUCT DISORDERS\",\n",
    "    \"DISSOCIATIVE DISORDERS\",\n",
    "    \"EATING DISORDERS\",\n",
    "    \"NEURO-COGNITIVE DISORDERS\",\n",
    "    \"NEURO-DEVELOPMENTAL DISORDERS\",\n",
    "    \"OBSESSIVE-COMPULSIVE AND RELATED DISORDERS\",\n",
    "    \"PERSONALITY DISORDERS\",\n",
    "    \"PSYCHEDELIC DRUGS\",\n",
    "    \"SCHIZOPHRENIA SPECTRUM AND OTHER PSYCHOTIC DISORDERS\",\n",
    "    \"SEXUAL DYSFUNCTIONS\",\n",
    "    \"SLEEP-WAKE DISORDERS\",\n",
    "    \"SOMATIC SYMPTOM RELATED DISORDERS\",\n",
    "    \"SUBSTANCE-RELATED DISORDERS\",\n",
    "    \"SYMPTOMS\",\n",
    "    \"TRAUMA AND STRESS RELATED DISORDERS\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_rel_feature(row):\n",
    "    def _extract_relations(text):\n",
    "        relations = []\n",
    "        if not isinstance(text, str):\n",
    "            return relations\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ in [\"nsubj\", \"dobj\"]:\n",
    "                    relations.append((token.head.text, token.dep_, token.text))\n",
    "        return relations\n",
    "\n",
    "    title_relations = _extract_relations(row[\"title\"])\n",
    "    text_relations = _extract_relations(row[\"selftext\"])\n",
    "    all_relations = title_relations + text_relations\n",
    "    return all_relations\n",
    "\n",
    "\n",
    "def create_psylabel_feature(row):\n",
    "    def _extract_psy_labels(text):\n",
    "        mh_labels = {}\n",
    "        if not isinstance(text, str):\n",
    "            return mh_labels\n",
    "\n",
    "        doc = psy_ner(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in MH_NER:\n",
    "                if ent.label_ not in mh_labels:\n",
    "                    mh_labels[ent.label_] = set()\n",
    "                mh_labels[ent.label_].add(ent.text)\n",
    "        for label in mh_labels:\n",
    "            mh_labels[label] = list(mh_labels[label])\n",
    "\n",
    "        return mh_labels\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _extract_psy_labels(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "# See: https://github.com/Ejhfast/empath-client/tree/master\n",
    "EMPATH_CATS = [\n",
    "    \"help\",\n",
    "    \"violence\",\n",
    "    \"sleep\",\n",
    "    \"medical_emergency\",\n",
    "    \"cold\",\n",
    "    \"hate\",\n",
    "    \"cheerfulness\",\n",
    "    \"aggression\",\n",
    "    \"envy\",\n",
    "    \"anticipation\",\n",
    "    \"health\",\n",
    "    \"pride\",\n",
    "    \"nervousness\",\n",
    "    \"weakness\",\n",
    "    \"horror\",\n",
    "    \"swearing_terms\",\n",
    "    \"suffering\",\n",
    "    \"sexual\",\n",
    "    \"fear\",\n",
    "    \"monster\",\n",
    "    \"irritability\",\n",
    "    \"exasperation\",\n",
    "    \"ridicule\",\n",
    "    \"neglect\",\n",
    "    \"fight\",\n",
    "    \"dominant_personality\",\n",
    "    \"injury\",\n",
    "    \"rage\",\n",
    "    \"science\",\n",
    "    \"work\",\n",
    "    \"optimism\",\n",
    "    \"warmth\",\n",
    "    \"sadness\",\n",
    "    \"emotional\",\n",
    "    \"joy\",\n",
    "    \"shame\",\n",
    "    \"torment\",\n",
    "    \"anger\",\n",
    "    \"strength\",\n",
    "    \"ugliness\",\n",
    "    \"pain\",\n",
    "    \"negative_emotion\",\n",
    "    \"positive_emotion\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_sentiment_feature(row):\n",
    "    def _get_vader_sentiment(text):\n",
    "        score = sia.polarity_scores(text)\n",
    "        return score[\"compound\"] if score is not None else np.NaN\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _get_vader_sentiment(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "def create_emotional_categories_scores_feature(row):\n",
    "    def _get_empath_sentiment(text):\n",
    "        scores = lexicon.analyze(text, categories=EMPATH_CATS, normalize=True)\n",
    "        if scores is not None:\n",
    "            return {category: round(score, 2) for category, score in scores.items()}\n",
    "        else:\n",
    "            return {}  # Return an empty dictionary if scores is None\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _get_empath_sentiment(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "# def create_emotional_categories_scores_feature(row):\n",
    "#     def _get_empath_sentiment(text):\n",
    "#         scores = lexicon.analyze(text, categories=EMPATH_CATS, normalize=True)\n",
    "#         return {category: round(score, 2) for category, score in scores.items()}\n",
    "\n",
    "#     combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "#     combined_labels = _get_empath_sentiment(combined_text)\n",
    "#     return combined_labels\n",
    "\n",
    "\n",
    "df_subset[\"psy_labels\"] = df_subset.progress_apply(create_psylabel_feature, axis=1)\n",
    "df_subset[\"sentiment\"] = df_subset.progress_apply(create_sentiment_feature, axis=1)\n",
    "df_subset[\"emotional_categories\"] = df_subset.progress_apply(\n",
    "    create_emotional_categories_scores_feature, axis=1\n",
    ")\n",
    "df_subset[\"semantic_relationships\"] = df_subset.progress_apply(\n",
    "    create_rel_feature, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Cache it for further processing down the line.\n",
    "df_subset.to_csv(\"./data/SemanticsRel.csv\", index=False)\n",
    "df_subset[\n",
    "    [\n",
    "        \"title\",\n",
    "        \"selftext\",\n",
    "        \"psy_labels\",\n",
    "        \"sentiment\",\n",
    "        \"emotional_categories\",\n",
    "        \"semantic_relationships\",\n",
    "    ]\n",
    "].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Nodes\n",
    "\n",
    "Nodes have edges, attributes for MH disorders, wieghts and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "668031it [00:28, 23093.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Word1</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Word2</th>\n",
       "      <th>MHlabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BPD</td>\n",
       "      <td>ask</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>relationship</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BPD</td>\n",
       "      <td>ask</td>\n",
       "      <td>dobj</td>\n",
       "      <td>goals</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BPD</td>\n",
       "      <td>discouraged</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>spectrum</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BPD</td>\n",
       "      <td>discouraged</td>\n",
       "      <td>dobj</td>\n",
       "      <td>characteristics</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPD</td>\n",
       "      <td>wondering</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>levels</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BPD</td>\n",
       "      <td>experiencing</td>\n",
       "      <td>dobj</td>\n",
       "      <td>anger</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BPD</td>\n",
       "      <td>found</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>way</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BPD</td>\n",
       "      <td>found</td>\n",
       "      <td>dobj</td>\n",
       "      <td>blame</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BPD</td>\n",
       "      <td>find</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>understanding</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BPD</td>\n",
       "      <td>tend</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>ones</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BPD</td>\n",
       "      <td>tend</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>love</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BPD</td>\n",
       "      <td>extend</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>demonize</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BPD</td>\n",
       "      <td>lead</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>cases</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BPD</td>\n",
       "      <td>know</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>anger</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BPD</td>\n",
       "      <td>experience</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>guys</td>\n",
       "      <td>{'SYMPTOMS': ['anger']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subreddit         Word1 Dependency            Word2  \\\n",
       "0        BPD           ask      nsubj     relationship   \n",
       "1        BPD           ask       dobj            goals   \n",
       "2        BPD   discouraged      nsubj         spectrum   \n",
       "3        BPD   discouraged       dobj  characteristics   \n",
       "4        BPD     wondering      nsubj           levels   \n",
       "5        BPD  experiencing       dobj            anger   \n",
       "6        BPD         found      nsubj              way   \n",
       "7        BPD         found       dobj            blame   \n",
       "8        BPD          find      nsubj    understanding   \n",
       "9        BPD          tend      nsubj             ones   \n",
       "10       BPD          tend      nsubj             love   \n",
       "11       BPD        extend      nsubj         demonize   \n",
       "12       BPD          lead      nsubj            cases   \n",
       "13       BPD          know      nsubj            anger   \n",
       "14       BPD    experience      nsubj             guys   \n",
       "\n",
       "                   MHlabels  \n",
       "0                        {}  \n",
       "1                        {}  \n",
       "2   {'SYMPTOMS': ['anger']}  \n",
       "3   {'SYMPTOMS': ['anger']}  \n",
       "4   {'SYMPTOMS': ['anger']}  \n",
       "5   {'SYMPTOMS': ['anger']}  \n",
       "6   {'SYMPTOMS': ['anger']}  \n",
       "7   {'SYMPTOMS': ['anger']}  \n",
       "8   {'SYMPTOMS': ['anger']}  \n",
       "9   {'SYMPTOMS': ['anger']}  \n",
       "10  {'SYMPTOMS': ['anger']}  \n",
       "11  {'SYMPTOMS': ['anger']}  \n",
       "12  {'SYMPTOMS': ['anger']}  \n",
       "13  {'SYMPTOMS': ['anger']}  \n",
       "14  {'SYMPTOMS': ['anger']}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = []\n",
    "\n",
    "for idx, row in tqdm(df_subset.iterrows()):\n",
    "    subreddit = row[\"subreddit\"]\n",
    "    labels = row[\"psy_labels\"]\n",
    "    relationships = row[\"semantic_relationships\"]\n",
    "    for rel in relationships:\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "            word1, dep, word2 = rel\n",
    "            clean_data.append(\n",
    "                {\n",
    "                    \"Subreddit\": subreddit,\n",
    "                    \"Word1\": word1,\n",
    "                    \"Dependency\": dep,\n",
    "                    \"Word2\": word2,\n",
    "                    \"MHlabels\": labels,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "clean_df.to_csv(\"./data/GraphData.csv\", index=False)\n",
    "clean_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Subreddit_x    Word1 Dependency           Word2  Occurrence  \\\n",
      "0             Anxiety     able       dobj         anxiety          10   \n",
      "1             Anxiety     able       dobj          breath           4   \n",
      "2             Anxiety     able       dobj         breathe          14   \n",
      "3             Anxiety     able       dobj           sleep           4   \n",
      "4             Anxiety     able      nsubj         anxiety          12   \n",
      "...               ...      ...        ...             ...         ...   \n",
      "104521  schizophrenia    worms      nsubj            tear           5   \n",
      "104522  schizophrenia    worse      nsubj  hallucinations           6   \n",
      "104523  schizophrenia    worse      nsubj          voices           6   \n",
      "104524  schizophrenia    write       dobj   schizophrenia           4   \n",
      "104525  schizophrenia  writing       dobj   schizophrenia          12   \n",
      "\n",
      "        Subreddit_y  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 1  \n",
      "3                 1  \n",
      "4                 1  \n",
      "...             ...  \n",
      "104521            1  \n",
      "104522            1  \n",
      "104523            1  \n",
      "104524            1  \n",
      "104525            1  \n",
      "\n",
      "[104526 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = (\n",
    "    clean_df.groupby([\"Subreddit\", \"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Occurrence\")\n",
    ")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 are not in the English dictionary\n",
    "# filtered_relationships = multiple_subreddit_relationships[\n",
    "#     (multiple_subreddit_relationships['Word1'].isin(english_words)) &\n",
    "#     (multiple_subreddit_relationships['Word2'].isin(english_words))\n",
    "# ]\n",
    "\n",
    "# Define a regular expression pattern to match only alphanumeric words\n",
    "pattern = re.compile(r\"^[a-zA-Z]+$\")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 contain only alphanumeric characters\n",
    "filtered_relationships = grouped[\n",
    "    (grouped[\"Word1\"].str.match(pattern)) & (grouped[\"Word2\"].str.match(pattern))\n",
    "]\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = filtered_relationships[filtered_relationships[\"Occurrence\"] > 3]\n",
    "\n",
    "\n",
    "# Grouping the DataFrame by 'Word1', 'Dependency', and 'Word2' to count unique occurrences across subreddits\n",
    "relationship_counts = (\n",
    "    common_relationships.groupby([\"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .agg({\"Subreddit\": \"nunique\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter relationships occurring in only one subreddit\n",
    "single_subreddit_relationships = relationship_counts[\n",
    "    relationship_counts[\"Subreddit\"] == 1\n",
    "]\n",
    "\n",
    "# Merge to keep only the relationships occurring in one subreddit from common_relationships\n",
    "filtered_common_relationships = pd.merge(\n",
    "    common_relationships,\n",
    "    single_subreddit_relationships,\n",
    "    on=[\"Word1\", \"Dependency\", \"Word2\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(filtered_common_relationships)\n",
    "filtered_common_relationships.to_csv(\"SemanticsRelFiltered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries have word1 and word2 which are not actual words but numbers or mispelled words. Even though there was a chance that these were to be filtered due to occurrence should be more than 3, some weren't. At this point i was going to use the NLTK library to check for english words, but upon implementing, some important words like 'zyprexa' was eliminated since it's not included in the corpus. However this is an important entry and therefore should remain. a more crude filter, removing numbers only entries was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\"zyprexa\" in english_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
