{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Relationship Extraction and Feature Engineering\n",
    "\n",
    "By christian Spiteri Gauci and Adam Darmanin\n",
    "\n",
    "## Literature and References\n",
    "\n",
    "## Method\n",
    "\n",
    "* Cleaning of data - Remove special characters, punctuation, and extra whitespace\n",
    "* Tokenise using spacy\n",
    "* Lemmatize tokens and remove stopwords\n",
    "* extract relationships - using \"nsubj\", \"ROOT\", \"dobj\"\n",
    "* Clean the extracted relationships - remove any entries that inlcude only special characters or numbers\n",
    "* Remove entries that occur only once - and keep those that occur 3times or more in each subreddit \n",
    "* Remove entries that occur in more than one subreddit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "\n",
    "from lib.sanitze_util import clean_text_batch\n",
    "\n",
    "tqdm.pandas()  # for progressbase in DFs.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "\n",
    "english_words = set(nltk_words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. Clean stopwords\n",
    "2. Lemmatize\n",
    "3. Convert emoticons\n",
    "4. Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_3304\\2031721653.py:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mask = df_subset.applymap(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
      "Cleaning Pipeline Token:   0%|          | 0/668031 [00:00<?, ?it/s]g:\\My Drive\\1. Educ\\1. Masters\\Year 1\\ICS5111 - Mining Large Scale Data\\Personal Assignment\\gitrepo\\mining-large-scale-data\\lib\\sanitze_util.py:88: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "C:\\Users\\chris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Cleaning Pipeline Token:  92%|█████████▏| 616705/668031 [01:09<00:07, 7291.12it/s] g:\\My Drive\\1. Educ\\1. Masters\\Year 1\\ICS5111 - Mining Large Scale Data\\Personal Assignment\\gitrepo\\mining-large-scale-data\\lib\\sanitze_util.py:88: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Cleaning Pipeline Token: 100%|██████████| 668031/668031 [01:15<00:00, 8891.81it/s]\n",
      "Cleaning Pipeline Token: 100%|██████████| 668031/668031 [19:41<00:00, 565.30it/s] \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/mental_disorders_reddit.csv\")\n",
    "\n",
    "# Create a new DataFrame with specific columns\n",
    "columns_needed = [\"title\", \"selftext\", \"subreddit\"]\n",
    "df_subset = df[columns_needed].copy()\n",
    "\n",
    "# Drop anything not a full peice of text in this dataset.\n",
    "mask = df_subset.applymap(lambda x: isinstance(x, str) and x.strip() != \"\")\n",
    "df_subset = df_subset[mask[\"title\"] & mask[\"selftext\"]]\n",
    "df_subset[\"raw_title\"] = df_subset[\"title\"]\n",
    "df_subset[\"raw_selftext\"] = df_subset[\"selftext\"]\n",
    "df_subset[\"title\"] = clean_text_batch(df_subset[\"title\"].fillna(\"\").tolist())\n",
    "df_subset[\"selftext\"] = clean_text_batch(df_subset[\"selftext\"].fillna(\"\").tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "1. Semantic relations between words\n",
    "2. Mental health labels in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chris/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chris\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mempath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Empath\n\u001b[0;32m      8\u001b[0m lexicon \u001b[38;5;241m=\u001b[39m Empath()\n\u001b[1;32m----> 9\u001b[0m sia \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m MH_NER \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANXIETY DISORDERS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBIPOLAR DISORDERS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAUMA AND STRESS RELATED DISORDERS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m ]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_rel_feature\u001b[39m(row):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chris/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chris\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# borrowed from the research: https://github.com/zhukovanadezhda/psy-ner/tree/main\n",
    "# Following paper: https://www.researchgate.net/publication/358779855_Deep_Learning-based_Detection_of_Psychiatric_Attributes_from_German_Mental_Health_Records\n",
    "# see: https://spacy.io/usage/processing-pipelines\n",
    "psy_ner = spacy.load(\"./model/psy_ner\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from empath import Empath\n",
    "\n",
    "lexicon = Empath()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "MH_NER = [\n",
    "    \"ANXIETY DISORDERS\",\n",
    "    \"BIPOLAR DISORDERS\",\n",
    "    \"DEPRESSIVE DISORDERS\",\n",
    "    \"DISRUPTIVE IMPULSE-CONTROL, AND CONDUCT DISORDERS\",\n",
    "    \"DISSOCIATIVE DISORDERS\",\n",
    "    \"EATING DISORDERS\",\n",
    "    \"NEURO-COGNITIVE DISORDERS\",\n",
    "    \"NEURO-DEVELOPMENTAL DISORDERS\",\n",
    "    \"OBSESSIVE-COMPULSIVE AND RELATED DISORDERS\",\n",
    "    \"PERSONALITY DISORDERS\",\n",
    "    \"PSYCHEDELIC DRUGS\",\n",
    "    \"SCHIZOPHRENIA SPECTRUM AND OTHER PSYCHOTIC DISORDERS\",\n",
    "    \"SEXUAL DYSFUNCTIONS\",\n",
    "    \"SLEEP-WAKE DISORDERS\",\n",
    "    \"SOMATIC SYMPTOM RELATED DISORDERS\",\n",
    "    \"SUBSTANCE-RELATED DISORDERS\",\n",
    "    \"SYMPTOMS\",\n",
    "    \"TRAUMA AND STRESS RELATED DISORDERS\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_rel_feature(row):\n",
    "    def _extract_relations(text):\n",
    "        relations = []\n",
    "        if not isinstance(text, str):\n",
    "            return relations\n",
    "        doc = nlp(text)\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ in [\"nsubj\", \"dobj\"]:\n",
    "                    relations.append((token.head.text, token.dep_, token.text))\n",
    "\n",
    "        return relations\n",
    "\n",
    "    title_relations = _extract_relations(row[\"title\"])\n",
    "    text_relations = _extract_relations(row[\"selftext\"])\n",
    "    all_relations = title_relations + text_relations\n",
    "    return all_relations\n",
    "\n",
    "\n",
    "def create_psylabel_feature(row):\n",
    "    def _extract_psy_labels(text):\n",
    "        mh_labels = {}\n",
    "        if not isinstance(text, str):\n",
    "            return mh_labels\n",
    "\n",
    "        doc = psy_ner(text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in MH_NER:\n",
    "                if ent.label_ not in mh_labels:\n",
    "                    mh_labels[ent.label_] = set()\n",
    "                mh_labels[ent.label_].add(ent.text)\n",
    "        for label in mh_labels:\n",
    "            mh_labels[label] = list(mh_labels[label])\n",
    "\n",
    "        return mh_labels\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _extract_psy_labels(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "# See: https://github.com/Ejhfast/empath-client/tree/master\n",
    "EMPATH_CATS = [\n",
    "    \"help\",\n",
    "    \"violence\",\n",
    "    \"sleep\",\n",
    "    \"medical_emergency\",\n",
    "    \"cold\",\n",
    "    \"hate\",\n",
    "    \"cheerfulness\",\n",
    "    \"aggression\",\n",
    "    \"envy\",\n",
    "    \"anticipation\",\n",
    "    \"health\",\n",
    "    \"pride\",\n",
    "    \"nervousness\",\n",
    "    \"weakness\",\n",
    "    \"horror\",\n",
    "    \"swearing_terms\",\n",
    "    \"suffering\",\n",
    "    \"sexual\",\n",
    "    \"fear\",\n",
    "    \"monster\",\n",
    "    \"irritability\",\n",
    "    \"exasperation\",\n",
    "    \"ridicule\",\n",
    "    \"neglect\",\n",
    "    \"fight\",\n",
    "    \"dominant_personality\",\n",
    "    \"injury\",\n",
    "    \"rage\",\n",
    "    \"science\",\n",
    "    \"work\",\n",
    "    \"optimism\",\n",
    "    \"warmth\",\n",
    "    \"sadness\",\n",
    "    \"emotional\",\n",
    "    \"joy\",\n",
    "    \"shame\",\n",
    "    \"torment\",\n",
    "    \"anger\",\n",
    "    \"strength\",\n",
    "    \"ugliness\",\n",
    "    \"pain\",\n",
    "    \"negative_emotion\",\n",
    "    \"positive_emotion\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_sentiment_feature(row):\n",
    "    def _get_vader_sentiment(text):\n",
    "        score = sia.polarity_scores(text)\n",
    "        return score[\"compound\"] if score is not None else np.NaN\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _get_vader_sentiment(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "def create_emotional_categories_scores_feature(row):\n",
    "    def _get_empath_sentiment(text):\n",
    "        scores = lexicon.analyze(text, categories=EMPATH_CATS, normalize=True)\n",
    "        return {category: round(score, 2) for category, score in scores.items()}\n",
    "\n",
    "    combined_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "    combined_labels = _get_empath_sentiment(combined_text)\n",
    "    return combined_labels\n",
    "\n",
    "\n",
    "df_subset[\"psy_labels\"] = df_subset.progress_apply(create_psylabel_feature, axis=1)\n",
    "df_subset[\"sentiment\"] = df_subset.progress_apply(create_sentiment_feature, axis=1)\n",
    "df_subset[\"emotional_categories\"] = df_subset.progress_apply(\n",
    "    create_emotional_categories_scores_feature, axis=1\n",
    ")\n",
    "df_subset[\"semantic_relationships\"] = df_subset.progress_apply(\n",
    "    create_rel_feature, axis=1\n",
    ")\n",
    "\n",
    "# Cache it for further processing down the line.\n",
    "df_subset.to_csv(\"./data/SemanticsRel.csv\", index=False)\n",
    "df_subset[\n",
    "    [\n",
    "        \"title\",\n",
    "        \"selftext\",\n",
    "        \"psy_labels\",\n",
    "        \"sentiment\",\n",
    "        \"emotional_categories\",\n",
    "        \"semantic_relationships\",\n",
    "    ]\n",
    "].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Nodes\n",
    "\n",
    "Nodes have edges, attributes for MH disorders, wieghts and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'psy_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'psy_labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df_subset\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[0;32m      4\u001b[0m     subreddit \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubreddit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpsy_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m     relationships \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic_relationships\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rel \u001b[38;5;129;01min\u001b[39;00m relationships:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'psy_labels'"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "\n",
    "for idx, row in tqdm(df_subset.iterrows()):\n",
    "    subreddit = row[\"subreddit\"]\n",
    "    labels = row[\"psy_labels\"]\n",
    "    relationships = row[\"semantic_relationships\"]\n",
    "    for rel in relationships:\n",
    "        if len(rel) == 3:  # Ensure the tuple has three elements\n",
    "            word1, dep, word2 = rel\n",
    "            clean_data.append(\n",
    "                {\n",
    "                    \"Subreddit\": subreddit,\n",
    "                    \"Word1\": word1,\n",
    "                    \"Dependency\": dep,\n",
    "                    \"Word2\": word2,\n",
    "                    \"MHlabels\": labels,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Issue with relationship: {rel}\")\n",
    "\n",
    "clean_df = pd.DataFrame(clean_data)\n",
    "clean_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Subreddit_x      Word1 Dependency      Word2  Occurrence  Subreddit_y\n",
      "0           BPD     advice       ROOT     advice          12            1\n",
      "1           BPD    believe       ROOT    believe           5            1\n",
      "2           BPD        bpd       ROOT        bpd          23            1\n",
      "3           BPD      broke       ROOT      broke           4            1\n",
      "4           BPD       came       ROOT       came           4            1\n",
      "..          ...        ...        ...        ...         ...          ...\n",
      "96          BPD       want      nsubj       love           4            1\n",
      "97          BPD       want      nsubj     people           5            1\n",
      "98          BPD       went       ROOT       went           9            1\n",
      "99          BPD       wish       ROOT       wish           4            1\n",
      "100         BPD  wondering       ROOT  wondering           4            1\n",
      "\n",
      "[101 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Grouping the DataFrame by 'Subreddit', 'Word1', 'Dependency', and 'Word2' and counting occurrences\n",
    "grouped = (\n",
    "    clean_df.groupby([\"Subreddit\", \"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Occurrence\")\n",
    ")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 are not in the English dictionary\n",
    "# filtered_relationships = multiple_subreddit_relationships[\n",
    "#     (multiple_subreddit_relationships['Word1'].isin(english_words)) &\n",
    "#     (multiple_subreddit_relationships['Word2'].isin(english_words))\n",
    "# ]\n",
    "\n",
    "# Define a regular expression pattern to match only alphanumeric words\n",
    "pattern = re.compile(r\"^[a-zA-Z]+$\")\n",
    "\n",
    "# Filter out entries where Word1 and Word2 contain only alphanumeric characters\n",
    "filtered_relationships = grouped[\n",
    "    (grouped[\"Word1\"].str.match(pattern)) & (grouped[\"Word2\"].str.match(pattern))\n",
    "]\n",
    "\n",
    "# Filtering relationships occurring more than three times within each subreddit\n",
    "common_relationships = filtered_relationships[filtered_relationships[\"Occurrence\"] > 3]\n",
    "\n",
    "\n",
    "# Grouping the DataFrame by 'Word1', 'Dependency', and 'Word2' to count unique occurrences across subreddits\n",
    "relationship_counts = (\n",
    "    common_relationships.groupby([\"Word1\", \"Dependency\", \"Word2\"])\n",
    "    .agg({\"Subreddit\": \"nunique\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter relationships occurring in only one subreddit\n",
    "single_subreddit_relationships = relationship_counts[\n",
    "    relationship_counts[\"Subreddit\"] == 1\n",
    "]\n",
    "\n",
    "# Merge to keep only the relationships occurring in one subreddit from common_relationships\n",
    "filtered_common_relationships = pd.merge(\n",
    "    common_relationships,\n",
    "    single_subreddit_relationships,\n",
    "    on=[\"Word1\", \"Dependency\", \"Word2\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(filtered_common_relationships)\n",
    "filtered_common_relationships.to_csv(\"SemanticsRelFiltered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries have word1 and word2 which are not actual words but numbers or mispelled words. Even though there was a chance that these were to be filtered due to occurrence should be more than 3, some weren't. At this point i was going to use the NLTK library to check for english words, but upon implementing, some important words like 'zyprexa' was eliminated since it's not included in the corpus. However this is an important entry and therefore should remain. a more crude filter, removing numbers only entries was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\"zyprexa\" in english_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
